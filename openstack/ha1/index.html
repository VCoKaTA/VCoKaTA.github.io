<!DOCTYPE html>
<html class="no-js" lang="en-us" xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="description" content="Openstack集群之Vip高可用">
    <meta name="author" content="">

    <meta name="keyword" content="">
    <link rel="shortcut icon" href="/favicon.ico">

    <title>Openstack集群之Vip高可用 &middot; VCoKaTA</title>

   	
    
        <link rel="stylesheet" href="https://vcokata.github.io/css/theme/flatly.css">
    

    <script defer src="https://use.fontawesome.com/releases/v5.6.3/js/solid.js" integrity="sha384-F4BRNf3onawQt7LDHDJm/hwm3wBtbLIfGk1VSB/3nn3E+7Rox1YpYcKJMsmHBJIl" crossorigin="anonymous"></script>
    <script defer src="https://use.fontawesome.com/releases/v5.6.3/js/regular.js" integrity="sha384-V+AkgA1cZ+p3DRK63AHCaXvO68V7B5eHoxl7QVN21zftbkFn/sGAIVR7vmQL3Zhp" crossorigin="anonymous"></script>
    <script defer src="https://use.fontawesome.com/releases/v5.6.3/js/brands.js" integrity="sha384-VLgz+MgaFCnsFLiBwE3ItNouuqbWV2ZnIqfsA6QRHksEAQfgbcoaQ4PP0ZeS0zS5" crossorigin="anonymous"></script>
    <script defer src="https://use.fontawesome.com/releases/v5.6.3/js/fontawesome.js" integrity="sha384-treYPdjUrP4rW5q82SnECO7TPVAz4bpas16yuE9F5o7CeBn2YYw1yr5oC8s8Mf8t" crossorigin="anonymous"></script>

   	
   	<link rel="stylesheet" href="https://vcokata.github.io/css/style.css">


    
    <script src="https://vcokata.github.io/js/jquery.min-2.1.4.js"></script>
    <script src="https://vcokata.github.io/js/bootstrap.min-3.3.5.js"></script>
<script src="http://cdn.bootcss.com/highlight.js/9.0.0/highlight.min.js"></script><link href="http://cdn.bootcss.com/highlight.js/9.0.0/styles/default.min.css" rel="stylesheet"><script>hljs.initHighlightingOnLoad();</script>
    
    <link href="" rel="alternate" type="application/rss+xml" title="VCoKaTA" />
</head>
<body lang="en">
    
    <div class="container">
    <div class="row">
        <div class="navbar navbar-default navbar-inverse" role="navigation">
            <div class="navbar-header">
                <a class="navbar-brand" href="https://vcokata.github.io">VCoKaTA</a>
            </div>
            <div class="navbar-collapse collapse navbar-responsive-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li><a href="https://vcokata.github.io/openstack/">Openstack</a></li>
                    <li><a href="https://vcokata.github.io/ceph/">Ceph</a></li>
                    <li><a href="https://vcokata.github.io/k8s/">Kubernetes</a></li>
                    <li><a href="https://vcokata.github.io/mesos/">Mesos</a></li>
                    <li><a href="https://vcokata.github.io/target/">Linux札记</a></li>
                    <li><a href="https://vcokata.github.io/note/">Python &amp; Django</a></li>
                                        <li><a href="https://vcokata.github.io/page/">个人</a></li>
                    
                </ul>
            </div>
        </div>
    </div>
</div>



<div>
    <div class="container col-md-9 ">
        <div class="row">
            <div class="col-md-offset-1 col-md-10">
                     using tags
                    
                        
                        <a href="/tags/%E9%AB%98%E5%8F%AF%E7%94%A8">高可用</a>
                    
                </small>
            </div>
        </div>
        <div class="row">
            <div class="col-md-offset-1 col-md-10">
                

                

<hr />

<p><font color=#DC143C size=5 face="黑体">openstack高可用官方推荐组件：Pacemaker+corosync+haproxy</font></p>

<pre><code>环境
系统centos7.2
三台主机 firewalld disable selinux disable
10.6.0.11 test-controller1
10.6.0.12 test-controller2
10.6.0.13 test-controller3
</code></pre>

<h3 id="1-安装软件">1.安装软件</h3>

<pre><code>相关软件 pacemaker pcs  corosync fence-agents resource-agents libqb0

yum install pcs pacemaker corosync fence-agents-all resource-agents -y
</code></pre>

<h2 id="2-启动pcsd服务">2.启动pcsd服务</h2>

<pre><code>systemctl enable pcsd
systemctl start pcsd
</code></pre>

<h2 id="3-创建集群用户">3.创建集群用户</h2>

<pre><code>安装pcsd时默认创建了用户hacluster
修改密码在所有集群节点上
echo 'Openstackcloud@#haproxy' | passwd --stdin hacluster
集群各节点之间进行认证

[root@test-controller2 ~]# pcs cluster auth test-controller1 test-controller2 test-controller3 -u hacluster -p Openstackcloud@#haproxy --force
test-controller3: Authorized
test-controller2: Authorized
test-controller1: Authorized

创建并启动名为openstack-cluste 的集群，其中controller1 controller2 为集群成员
[root@test-controller2 ~]#  pcs cluster setup --force --name openstack-cluster test-controller1 test-controller2 test-controller3
</code></pre>

<h2 id="4-设置集群自启动">4.设置集群自启动</h2>

<pre><code>[root@test-controller2 ~]# pcs cluster enable --all
test-controller1: Cluster Enabled
test-controller2: Cluster Enabled
test-controller3: Cluster Enabled
[root@test-controller2 ~]# pcs cluster start --all
test-controller3: Starting Cluster...
test-controller1: Starting Cluster...
test-controller2: Starting Cluster...

查看集群状态
[root@test-controller2 ~]# pcs cluster status
Cluster Status:
 Stack: unknown
 Current DC: NONE
 Last updated: Thu Apr  6 15:47:36 2017        Last change: Thu Apr  6 15:47:33 2017 by hacluster via crmd on test-controller3
 3 nodes and 0 resources configured
PCSD Status:
  test-controller1: Online
  test-controller3: Online
  test-controller2: Online

检查pacemaker服务
[root@test-controller2 ~]# ps aux | grep pacemaker
root     12285  0.0  0.0 130528  9220 ?        Ss   15:47   0:00 /usr/sbin/pacemakerd -f
haclust+ 12286  0.0  0.0 133168 15364 ?        Ss   15:47   0:00 /usr/libexec/pacemaker/cib
root     12287  0.0  0.0 133500  7928 ?        Ss   15:47   0:00 /usr/libexec/pacemaker/stonithd
root     12288  0.0  0.0 102992  5028 ?        Ss   15:47   0:00 /usr/libexec/pacemaker/lrmd
haclust+ 12289  0.0  0.0 124820  7644 ?        Ss   15:47   0:00 /usr/libexec/pacemaker/attrd
haclust+ 12290  0.0  0.0 114940  4556 ?        Ss   15:47   0:00 /usr/libexec/pacemaker/pengine
haclust+ 12291  0.0  0.0 143228  9004 ?        Ss   15:47   0:00 /usr/libexec/pacemaker/crmd
root     12643  0.0  0.0 112648   976 pts/2    S+   15:49   0:00 grep --color=auto pacemaker

Start Corosync
配置
cp  /etc/corosync/corosync.confexample /etc/corosync/corosync.conf
cat /etc/corosync/corosync.conf|grep -v '#'
totem {
    version: 2
    secauth: on
    threads: 0
    crypto_cipher: none
    crypto_hash: none
    interface {
        ringnumber: 0
        bindnetaddr: 10.6.1.0
        mcastaddr: 239.255.200.1
        mcastport: 5405
        ttl: 1
    }
}
amf {
     mode: disabled
}
service {
    ver:       1
        name:      pacemaker
}
aisexec {
        user:   root
        group:  root
}
logging {
    fileline: off
    to_stderr: no
    to_logfile: yes
    logfile: /var/log/cluster/corosync.log
    to_syslog: yes
    debug: off
    timestamp: on
    logger_subsys {
        subsys: AFM
        debug: off
        tags: enter|leave|trace1|trace2|trace3|trace4|trace6
    }
}
quorum {
        provider: corosync_votequorum
}

[root@test-controller1 corosync]# corosync-keygen
Corosync Cluster Engine Authentication key generator.
Gathering 1024 bits for key from /dev/random.
Press keys on your keyboard to generate entropy.
Press keys on your keyboard to generate entropy (bits = 624).
Press keys on your keyboard to generate entropy (bits = 920).
Press keys on your keyboard to generate entropy (bits = 1000).
Writing corosync key to /etc/corosync/authkey.

cd /etc/corosync/
scp -p authkey corosync.conf 10.6.0.12:/etc/corosync/
scp -p authkey corosync.conf 10.6.0.13:/etc/corosync/

启动
 systemctl enable corosync
 systemctl start corosync

 pcs cluster status
Cluster Status:
 Stack: corosync
 Current DC: test-controller1 (version 1.1.15-11.el7_3.4-e174ec8) - partition with quorum
 Last updated: Thu Apr  6 15:55:47 2017        Last change: Thu Apr  6 15:47:54 2017 by hacluster via crmd on test-controller1
 3 nodes and 0 resources configured
PCSD Status:
  test-controller2: Online
  test-controller3: Online
  test-controller1: Online

查看状态
corosync-cfgtool -s
Printing ring status.
Local node ID 1
RING ID 0
    id    = 10.6.0.11
    status    = ring 0 active with no faults
[root@test-controller2 ~]# corosync-cfgtool -s
Printing ring status.
Local node ID 2
RING ID 0
    id    = 10.6.0.12
    status    = ring 0 active with no faults
[root@test-controller3 corosync]# corosync-cfgtool -s
Printing ring status.
Local node ID 3
RING ID 0
    id    = 10.6.0.13
    status    = ring 0 active with no faults

Start Pacemaker
 systemctl enable pacemaker
 systemctl start pacemaker

查看Pacemaker 状态
[root@test-controller2 ~]# crm_mon -1
Stack: corosync
Current DC: test-controller1 (version 1.1.15-11.el7_3.4-e174ec8) - partition with quorum
Last updated: Thu Apr  6 17:15:04 2017        Last change: Thu Apr  6 15:47:54 2017 by hacluster via crmd on test-controller1
3 nodes and 0 resources configured
Online: [ test-controller1 test-controller2 test-controller3 ]
No active resources

检验Corosync的安装及当前corosync状态：
corosync-cfgtool -s
corosync-cmapctl | grep members
pcs status corosync

检验Corosync的安装及当前corosync状态：
[root@test-controller2 ~]# crm_verify -L -V
   error: unpack_resources:    Resource start-up disabled since no STONITH resources have been defined
   error: unpack_resources:    Either configure some or disable STONITH with the stonith-enabled option
   error: unpack_resources:    NOTE: Clusters with shared data need STONITH to ensure data integrity
Errors found during check: config not valid

[root@test-controller2 ~]# pcs property set stonith-enabled=false
[root@test-controller2 ~]# pcs property set no-quorum-policy=ignore
[root@test-controller2 ~]# crm_verify -L -V
</code></pre>

<h2 id="5-设置基本的群集属性">5.设置基本的群集属性</h2>

<pre><code> pcs property set pe-warn-series-max=1000  pe-input-series-max=1000   pe-error-series-max=1000  cluster-recheck-interval=5min
Configure the VIP
pcs resource create vip ocf:heartbeat:IPaddr2   params ip=&quot;10.6.0.10&quot; cidr_netmask=&quot;24&quot; op monitor interval=&quot;30s&quot;

[root@test-controller2 ~]# crm_mon -1
Stack: corosync
Current DC: test-controller1 (version 1.1.15-11.el7_3.4-e174ec8) - partition with quorum
Last updated: Thu Apr  6 17:36:19 2017        Last change: Thu Apr  6 17:33:43 2017 by root via cibadmin on test-controller2
3 nodes and 1 resource configured
Online: [ test-controller1 test-controller2 test-controller3 ]
Active resources:
 vip    (ocf::heartbeat:IPaddr2):    Started test-controller1

安装haproxy
yum install haproxy -y  
echo net.ipv4.ip_nonlocal_bind = 1 &gt;&gt;/etc/sysctl.conf
sysctl -p

需要开启这项参数，节点才能监听其他机器ip
配置资源约束
pcs resource create lb-haproxy systemd:haproxy --clone

定义约束，先启动VIP之后才启动HAProxy
pcs constraint order start vip then lb-haproxy-clone kind=Optional

定义运行的HAProxy和VIP必须在同一节点上
pcs constraint colocation add lb-haproxy-clone with vip

[root@test-controller2 ~]# crm_mon -1
Stack: corosync
Current DC: test-controller1 (version 1.1.15-11.el7_3.4-e174ec8) - partition with quorum
Last updated: Thu Apr  6 17:50:22 2017        Last change: Thu Apr  6 17:48:48 2017 by root via cibadmin on test-controller2
3 nodes and 4 resources configured
Online: [ test-controller1 test-controller2 test-controller3 ]
Active resources:
 vip    (ocf::heartbeat:IPaddr2):    Started test-controller1
 Clone Set: lb-haproxy-clone [lb-haproxy]
     Started: [ test-controller1 ]
Failed Actions:
* lb-haproxy_start_0 on test-controller2 'not running' (7): call=11, status=complete, exitreason='none',
    last-rc-change='Thu Apr  6 17:48:47 2017', queued=0ms, exec=2055ms
</code></pre>

<h2 id="6-haproxy配置">6.haproxy配置</h2>

<pre><code> global
  chroot  /var/lib/haproxy
  log 127.0.0.1 local3
  daemon
  group  haproxy
  maxconn  4000
  pidfile  /var/run/haproxy.pid
  user  haproxy
defaults
  log  global
  maxconn  4000
  option  redispatch
  retries  3
  timeout  http-request 10s
  timeout  queue 1m
  timeout  connect 10s
  timeout  client 1m
  timeout  server 1m
  timeout  check 10s
 listen dashboard_cluster
  bind 10.6.0.10:80
  balance  source
  option  tcpka
  option  httpchk
  option  tcplog
  server controller1 10.6.0.11:80 check inter 2000 rise 2 fall 5
  server controller2 10.6.0.12:80 check inter 2000 rise 2 fall 5
  server controller3 10.6.0.13:80 check inter 2000 rise 2 fall 5
 listen galera_cluster
  bind 10.6.0.10:3306
  balance  source
  option  mysql-check
  server controller1 10.6.0.11:3306 check port 9200 inter 2000 rise 2 fall 5
  server controller2 10.6.0.12:3306 backup check port 9200 inter 2000 rise 2 fall 5
  server controller3 10.6.0.13:3306 backup check port 9200 inter 2000 rise 2 fall 5
 listen glance_api_cluster
  bind 10.6.0.10:9292
  balance  source
  option  tcpka
  option  httpchk
  option  tcplog
  server controller1 10.6.0.11:9292 check inter 2000 rise 2 fall 5
  server controller2 10.6.0.12:9292 check inter 2000 rise 2 fall 5
  server controller3 10.6.0.13:9292 check inter 2000 rise 2 fall 5
 listen glance_registry_cluster
  bind 10.6.0.10:9191
  balance  source
  option  tcpka
  option  tcplog
  server controller1 10.6.0.11:9191 check inter 2000 rise 2 fall 5
  server controller2 10.6.0.12:9191 check inter 2000 rise 2 fall 5
  server controller3 10.6.0.13:9191 check inter 2000 rise 2 fall 5
 listen keystone_admin_cluster
  bind 10.6.0.10:35357
  balance  source
  option  tcpka
  option  httpchk
  option  tcplog
  server controller1 10.6.0.11:35357 check inter 2000 rise 2 fall 5
  server controller2 10.6.0.12:35357 check inter 2000 rise 2 fall 5
  server controller3 10.6.0.13:35357 check inter 2000 rise 2 fall 5
 listen keystone_public_internal_cluster
  bind 10.6.0.10:5000
  balance  source
  option  tcpka
  option  httpchk
  option  tcplog
  server controller1 10.6.0.11:5000 check inter 2000 rise 2 fall 5
  server controller2 10.6.0.12:5000 check inter 2000 rise 2 fall 5
  server controller3 10.6.0.13:5000 check inter 2000 rise 2 fall 5
# listen nova_ec2_api_cluster
#  bind 10.6.0.10:8773
#  balance  source
#  option  tcpka
#  option  tcplog
#  server controller1 10.6.0.11:8773 check inter 2000 rise 2 fall 5
#  server controller2 10.6.0.12:8773 check inter 2000 rise 2 fall 5
#  server controller3 10.6.0.13:8773 check inter 2000 rise 2 fall 5
 listen nova_compute_api_cluster
  bind 10.6.0.10:8774
  balance  source
  option  tcpka
  option  httpchk
  option  tcplog
  server controller1 10.6.0.11:8774 check inter 2000 rise 2 fall 5
  server controller2 10.6.0.12:8774 check inter 2000 rise 2 fall 5
  server controller3 10.6.0.13:8774 check inter 2000 rise 2 fall 5
 listen nova_metadata_api_cluster
  bind 10.6.0.10:8775
  balance  source
  option  tcpka
  option  tcplog
  server controller1 10.6.0.11:8775 check inter 2000 rise 2 fall 5
  server controller2 10.6.0.12:8775 check inter 2000 rise 2 fall 5
  server controller3 10.6.0.13:8775 check inter 2000 rise 2 fall 5
 listen cinder_api_cluster
  bind 10.6.0.10:8776
  balance  source
  option  tcpka
  option  httpchk
  option  tcplog
  server controller1 10.6.0.11:8776 check inter 2000 rise 2 fall 5
  server controller2 10.6.0.12:8776 check inter 2000 rise 2 fall 5
  server controller3 10.6.0.13:8776 check inter 2000 rise 2 fall 5
 listen ceilometer_api_cluster
  bind 10.6.0.10:8777
  balance  source
  option  tcpka
  option  tcplog
  server controller1 10.6.0.11:8777 check inter 2000 rise 2 fall 5
  server controller2 10.6.0.12:8777 check inter 2000 rise 2 fall 5
  server controller3 10.6.0.13:8777 check inter 2000 rise 2 fall 5
 listen nova_vncproxy_cluster
  bind 10.6.0.10:6080
  balance  source
  option  tcpka
  option  tcplog
  server controller1 10.6.0.11:6080 check inter 2000 rise 2 fall 5
  server controller2 10.6.0.12:6080 check inter 2000 rise 2 fall 5
  server controller3 10.6.0.13:6080 check inter 2000 rise 2 fall 5
 listen neutron_api_cluster
  bind 10.6.0.10:9696
  balance  source
  option  tcpka
  option  httpchk
  option  tcplog
  server controller1 10.6.0.11:9696 check inter 2000 rise 2 fall 5
  server controller2 10.6.0.12:9696 check inter 2000 rise 2 fall 5
  server controller3 10.6.0.13:9696 check inter 2000 rise 2 fall 5
 listen swift_proxy_cluster
  bind 10.6.0.10:8080
  balance  source
  option  tcplog
  option  tcpka
  server controller1 10.6.0.11:8080 check inter 2000 rise 2 fall 5
  server controller2 10.6.0.12:8080 check inter 2000 rise 2 fall 5
  server controller3 10.6.0.13:8080 check inter 2000 rise 2 fall 5
systemctl enable haproxy
systemctl start haproxy
注意：以上端口要监听本地ip的端口，不能监听所有
</code></pre>

<p><font color=#DC143C size=5 face="黑体">我这里图简单，用的keepalived+haproxy替换了。感觉还是官方推荐的比较稳健，可以设置很多策略，比如启动顺序，vip必须与haproxy在一起，这样haproxy挂了后，vip自己切走，keepalived 需要自己写脚本完成。</font></p>

            </div>
        </div>
        <div class="row">
            <div class="col-md-12">
                <hr>
            </div>
        </div>
        <section id="comments" class="themeform">
<div class="ds-thread" data-thread-key="/openstack/ha1/" data-title="Openstack集群之Vip高可用" data-url="https://vcokata.github.io/openstack/ha1/"></div>
<script type="text/javascript">
var duoshuoQuery = {short_name:"Zen"};
    (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] 
         || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
</script>
</section>
    </div>
    <div class="col-md-3 panel panel-success">
         <ul id="tree" class="ztree" style="list-style-type:none"></ul>
    </div>
</div>
<link rel="stylesheet" href="https://vcokata.github.io/css/zTreeStyle.css" type="text/css"><script src="https://vcokata.github.io/js/jquery.ztree.core-3.5.min.js"></script><script src="https://vcokata.github.io/js/ztree_toc.js"></script><script>
$(document).ready(function(){
    $('#tree').ztree_toc({
        is_posion_top:false,
        is_expand_all: true
    });
});
</script>
    <div class="container">
        <div class="row col-md-12">
            <footer>
                <div class="pull-left">
                    <p>
                        &copy;  ~ Powered By <a href="http://hugo.spf13.com">Hugo</a> - version: 0.58.3 ~ <a href="https://vcokata.github.io/license">License</a>
                    </p>
                </div>

                
                <div class="pull-right">
                    
                    
                    
                    
                        <a href="https://github.com/VCoKaTA" target="_blank">
                        <span class="sr-only">GitHub profile</span>
                        <i class="fab fa-github-square fa-2x"></i></a>
                    
                    
                    
                    
                    
                </div>
            </footer>
        </div>
    </div>

    
    </body>
</html>


