<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>K8s on SomethingCM</title>
    <link>https://SomethingCM.github.io/k8s/</link>
    <description>Recent content in K8s on SomethingCM</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Oct 2018 01:19:14 +0800</lastBuildDate>
    
	<atom:link href="https://SomethingCM.github.io/k8s/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>K8s Harbor镜像清理</title>
      <link>https://SomethingCM.github.io/k8s/harbor2/</link>
      <pubDate>Fri, 19 Oct 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/harbor2/</guid>
      <description>每个项目我只保留最近10个镜像 cat harbor_del.py #! /usr/bin/env python # -*- coding:utf-8 -*- import json import requests from pprint import pprint url=&amp;quot;xxx.xxx.xxx.7&amp;quot; username=&amp;quot;admin&amp;quot; password=&amp;quot;xxx&amp;quot; def get_projects(): try: r = requests.get(&amp;quot;https://&amp;quot;+url+&amp;quot;/api/projects&amp;quot;, auth=(username,password),headers={&amp;quot;Content-Type&amp;quot;: &amp;quot;application/json&amp;quot;}, verify=False) return r.json() except Exception as e: print(str(e)) def get_project_info(pro_id): try: r = requests.get(&amp;quot;https://&amp;quot;+url+&amp;quot;/api/repositories&amp;quot;, auth=(username,password),params={&amp;quot;project_id&amp;quot;:pro_id},headers={&amp;quot;Content-Type&amp;quot;: &amp;quot;application/json&amp;quot;}, verify=False) return r.json() except Exception as e: print(str(e)) def del_0_size_repos(repo_name): del_reps=[] r = requests.get(&amp;quot;https://&amp;quot;+url+&amp;quot;/api/repositories/&amp;quot;+repo_name+&amp;quot;/tags&amp;quot;, auth=(username,password),headers={&amp;quot;Content-Type&amp;quot;: &amp;quot;application/json&amp;quot;}, verify=False) tags = r.json() for i in tags: if i[&#39;size&#39;] == 0: rr=requests.</description>
    </item>
    
    <item>
      <title>K8s 二进制部署8.1（部署 coredns 插件）</title>
      <link>https://SomethingCM.github.io/k8s/k8s8.1/</link>
      <pubDate>Thu, 18 Oct 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/k8s8.1/</guid>
      <description>修改配置文件 将下载的 kubernetes-server-linux-amd64.tar.gz 解压后，再解压其中的 kubernetes-src.tar.gz 文件。 cd kubernetes/ tar -xzvf kubernetes-src.tar.gz coredns 目录是 cluster/addons/dns： cd cluster/addons/dns/coredns cp coredns.yaml.base coredns.yaml export CLUSTER_DNS_DOMAIN=&amp;quot;cluster.local&amp;quot; export CLUSTER_DNS_SVC_IP=10.68.0.2 sed -i -e &amp;quot;s/__PILLAR__DNS__DOMAIN__/${CLUSTER_DNS_DOMAIN}/&amp;quot; -e &amp;quot;s/__PILLAR__DNS__SERVER__/${CLUSTER_DNS_SVC_IP}/&amp;quot; coredns.yaml 镜像用国内的： image: registry.aliyuncs.com/google_containers/coredns:1.3.1 内存资源限制自定义 resources: limits: memory: 200Mi 我起了3副本 replicas: 3 [root@k8s-1 coredns]# cat coredns.yaml # __MACHINE_GENERATED_WARNING__ apiVersion: v1 kind: ServiceAccount metadata: name: coredns namespace: kube-system labels: kubernetes.io/cluster-service: &amp;quot;true&amp;quot; addonmanager.kubernetes.io/mode: Reconcile --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: kubernetes.io/bootstrapping: rbac-defaults addonmanager.</description>
    </item>
    
    <item>
      <title>K8s 二进制部署8.2（部署 Dashboard）</title>
      <link>https://SomethingCM.github.io/k8s/k8s8.2/</link>
      <pubDate>Thu, 18 Oct 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/k8s8.2/</guid>
      <description>1、下载配置dashboard-service.yaml 将下载的 kubernetes-server-linux-amd64.tar.gz 解压后，再解压其中的 kubernetes-src.tar.gz 文件。 dashboard 对应的目录是：cluster/addons/dashboard cd /root/kubernetes/cluster/addons/dashboard 修改 service 定义，指定端口类型为 NodePort，这样外界可以通过地址 NodeIP:NodePort 访问 dashboard； [root@k8s-1 dashboard]# cat dashboard-service.yaml apiVersion: v1 kind: Service metadata: name: kubernetes-dashboard namespace: kube-system labels: k8s-app: kubernetes-dashboard kubernetes.io/cluster-service: &amp;quot;true&amp;quot; addonmanager.kubernetes.io/mode: Reconcile spec: type: NodePort selector: k8s-app: kubernetes-dashboard ports: - port: 443 targetPort: 8443 vim dashboard-controller.yaml image: registry.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.10.1 执行所有定义文件 [root@k8s-1 dashboard]# ls *.yaml dashboard-configmap.yaml dashboard-controller.yaml dashboard-rbac.yaml dashboard-secret.yaml dashboard-service.yaml [root@k8s-1 dashboard]# cat *.yaml apiVersion: v1 kind: ConfigMap metadata: labels: k8s-app: kubernetes-dashboard # Allows editing resource and makes sure it is created first.</description>
    </item>
    
    <item>
      <title>K8s 二进制部署8.3（部署 metrics-server 插件）</title>
      <link>https://SomethingCM.github.io/k8s/k8s8.3/</link>
      <pubDate>Thu, 18 Oct 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/k8s8.3/</guid>
      <description>集群部署好后，如果我们想知道集群中每个节点及节点上的pod资源使用情况，命令行下可以直接使用kubectl top node/pod来查看资源使用情况，默认此命令不能正常使用，需要我们部署对应api资源才可以使用此命令。从 Kubernetes 1.8 开始，资源使用指标（如容器 CPU 和内存使用率）通过 Metrics API 在 Kubernetes 中获取, metrics-server 替代了heapster。Metrics Server 实现了Resource Metrics API，Metrics Server 是集群范围资源使用数据的聚合器。 Metrics Server 从每个节点上的 Kubelet 公开的 Summary API 中采集指标信息。heapster从1.13版本开始被废弃，官方推荐使用Metrics Server+Prometheus方案进行集群监控。 metrics-server 通过 kube-apiserver 发现所有节点，然后调用 kubelet APIs（通过 https 接口）获得各节点（Node）和 Pod 的 CPU、Memory 等资源使用情况。
1、安装 metrics-server 下载资源清单配置文件(6个) 官方项目地址：https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/metrics-server 生产稳定版 https://github.com/kubernetes-incubator/metrics-server/tree/master/deploy/1.8%2B [root@k8s-1 ~]# mkdir metrics-server [root@k8s-1 ~]# cd metrics-server [root@k8s-1 metrics-server]#for file in auth-delegator.yaml auth-reader.yaml metrics-apiservice.yaml metrics-server-deployment.yaml metrics-server-service.yaml resource-reader.yaml;do wget https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.15/cluster/addons/metrics-server/$file;done 本次使用生产稳定版 [root@k8s-1 metrics-server]# for file in auth-delegator.</description>
    </item>
    
    <item>
      <title>K8s 二进制部署8.4（部署 traefik ingress）</title>
      <link>https://SomethingCM.github.io/k8s/k8s8.4/</link>
      <pubDate>Thu, 18 Oct 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/k8s8.4/</guid>
      <description>设置节点标签 kubectl label nodes 10.200.46.42 edgenode=true kubectl get nodes --show-labels cat traefik-rbac.yaml kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: traefik-ingress-controller rules: - apiGroups: - &amp;quot;&amp;quot; resources: - services - endpoints - secrets verbs: - get - list - watch - apiGroups: - extensions resources: - ingresses verbs: - get - list - watch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: traefik-ingress-controller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: traefik-ingress-controller subjects: - kind: ServiceAccount name: traefik-ingress-controller namespace: kube-system cat traefik.</description>
    </item>
    
    <item>
      <title>K8s 二进制部署8.5（部署 Prometheus-operator监控）</title>
      <link>https://SomethingCM.github.io/k8s/k8s8.5/</link>
      <pubDate>Thu, 18 Oct 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/k8s8.5/</guid>
      <description>Prometheus Operator不同于Prometheus，Prometheus Operator是 CoreOS 开源的一套用于管理在 Kubernetes 集群上的 Prometheus 控制器，它是为了简化在 Kubernetes 上部署、管理和运行 Prometheus 和 Alertmanager 集群。
官方提供的架构图
kubernetes也在官方的github上关于使用prometheus监控的建议： 地址：https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/prometheus  1、相关服务说明 　Operator： Operator 资源会根据自定义资源（Custom Resource Definition / CRDs）来部署和管理 Prometheus Server，同时监控这些自定义资源事件的变化来做相应的处理，是整个系统的控制中心。 Prometheus： Prometheus 资源是声明性地描述 Prometheus 部署的期望状态。 Prometheus Server： Operator 根据自定义资源 Prometheus 类型中定义的内容而部署的 Prometheus Server 集群，这些自定义资源可以看作是用来管理 Prometheus Server 集群的 StatefulSets 资源。 ServiceMonitor： ServiceMonitor 也是一个自定义资源，它描述了一组被 Prometheus 监控的 targets 列表。该资源通过 Labels 来选取对应的 Service Endpoint，让 Prometheus Server 通过选取的 Service 来获取 Metrics 信息。 Service： Service 资源主要用来对应 Kubernetes 集群中的 Metrics Server Pod，来提供给 ServiceMonitor 选取让 Prometheus Server 来获取信息。简单的说就是 Prometheus 监控的对象，例如之前了解的 Node Exporter Service、Mysql Exporter Service 等等。 Alertmanager： Alertmanager 也是一个自定义资源类型，由 Operator 根据资源描述内容来部署 Alertmanager 集群  2、下载部署 2.</description>
    </item>
    
    <item>
      <title>K8s 二进制部署8.6（部署 EFK 日志系统）</title>
      <link>https://SomethingCM.github.io/k8s/k8s8.6/</link>
      <pubDate>Thu, 18 Oct 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/k8s8.6/</guid>
      <description>1、下载配置 官方：
https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch 1：fluentd-es.ds.yaml配置中pod有对node的label进行选择调度，而我们集群中的node 是没有这个label，所以需要我们手工添加每个节点的label来保证调度的成功，执行如下命令： kubectl label nodes 节点名称 beta.kubernetes.io/fluentd-ds-ready=true 节点名可以通过 kubectl get nodes --show-labels 来获取  2、调整配置 kibana-deployment.yaml中的指定资源使用的api和的路劲和我这个版本的api的路径不一样，导致后面访问不了ui（kibana）的界面，不知道的可以执行如下命令来获取自己相应版本的大致路径,然后修改下就可以了(看了下官方现在默认的是和我k8s是对的上的) [root@k8s-1 ~]# kubectl cluster-info Kubernetes master is running at https://10.200.46.44:6443 CoreDNS is running at https://10.200.46.44:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy kubernetes-dashboard is running at https://10.200.46.44:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy Metrics-server is running at https://10.200.46.44:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy 直接注释掉可以直接访问kibana  3、安装EFK kubectl apply -f fluentd-es-configmap.yaml kubectl apply -f fluentd-es-ds.yaml kubectl apply -f es-statefulset.yaml kubectl apply -f es-service.yaml kubectl apply -f kibana-deployment.yaml kubectl apply -f kibana-service.</description>
    </item>
    
    <item>
      <title>K8s 二进制部署7（部署calico网络）</title>
      <link>https://SomethingCM.github.io/k8s/k8s7/</link>
      <pubDate>Thu, 11 Oct 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/k8s7/</guid>
      <description>1、为什么node节点处于not ready的状态呢？ 我们可以通过日志观察一下： [root@k8s-1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION 10.200.46.39 Ready &amp;lt;none&amp;gt; 3h5m v1.15.0 10.200.46.40 Ready &amp;lt;none&amp;gt; 3h15m v1.15.0 10.200.46.41 Ready &amp;lt;none&amp;gt; 3h4m v1.15.0 10.200.46.42 Ready &amp;lt;none&amp;gt; 178m v1.15.0 10.200.46.43 Ready &amp;lt;none&amp;gt; 178m v1.15.0 [root@k8s-1 kubelet]# tail -f kubelet.WARNING Running on machine: k8s-1 Binary: Built with gc go1.12.5 for linux/amd64 Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg W0925 14:25:51.202678 3710 docker_service.go:561] Hairpin mode set to &amp;quot;promiscuous-bridge&amp;quot; but kubenet is not enabled, falling back to &amp;quot;hairpin-veth&amp;quot; W0925 14:25:51.</description>
    </item>
    
    <item>
      <title>K8s 二进制部署8（部署集群插件）</title>
      <link>https://SomethingCM.github.io/k8s/k8s8/</link>
      <pubDate>Thu, 11 Oct 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/k8s8/</guid>
      <description>插件是集群的附件组件，丰富和完善了集群的功能。 8-1.coredns 8-2.Dashboard 8-3.Metrics Server 8-4.部署 traefik ingress 8-5.k8s监控（prometheus+grafana+alertmanager） 8-6.EFK (elasticsearch、fluentd、kibana)  </description>
    </item>
    
    <item>
      <title>K8s 二进制部署6.1（部署worker节点）</title>
      <link>https://SomethingCM.github.io/k8s/k8s6.1/</link>
      <pubDate>Wed, 10 Oct 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/k8s6.1/</guid>
      <description>初始化已安装 参考配置 cat &amp;gt; docker.service &amp;lt;&amp;lt;&amp;quot;EOF&amp;quot; [Unit] Description=Docker Application Container Engine Documentation=http://docs.docker.io [Service] WorkingDirectory=##DOCKER_DIR## ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS ExecReload=/bin/kill -s HUP $MAINPID Restart=on-failure RestartSec=5 LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity Delegate=yes KillMode=process [Install] WantedBy=multi-user.target EOF # EOF 前后有双引号，这样 bash 不会替换文档中的变量，如 $DOCKER_NETWORK_OPTIONS (这些环境变量是 systemd 负责替换的。)； # dockerd 运行时会调用其它 docker 命令，如 docker-proxy，所以需要将 docker 命令所在的目录加到 PATH 环境变量中； # docker 需要以 root 用于运行； # docker 从 1.13 版本开始，可能将 iptables FORWARD chain的默认策略设置为DROP，从而导致 ping 其它 Node 上的 Pod IP 失败，遇到这种情况时，需要手动设置策略为 ACCEPT： iptables -P FORWARD ACCEPT 并且把以下命令写入 /etc/rc.</description>
    </item>
    
    <item>
      <title>K8s 二进制部署6.2（部署kubelet组件）</title>
      <link>https://SomethingCM.github.io/k8s/k8s6.2/</link>
      <pubDate>Wed, 10 Oct 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/k8s6.2/</guid>
      <description>1、创建 kubelet bootstrap kubeconfig 文件 创建 token export BOOTSTRAP_TOKEN=$(kubeadm token create \ --description kubelet-bootstrap-token \ --groups system:bootstrappers:node \ --kubeconfig ~/.kube/config) kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/pki/ca.pem \ --embed-certs=true \ --server=https://10.200.46.44:6443 \ --kubeconfig=kubelet-bootstrap.kubeconfig # 设置客户端认证参数 kubectl config set-credentials kubelet-bootstrap \ --token=${BOOTSTRAP_TOKEN} \ --kubeconfig=kubelet-bootstrap.kubeconfig # 设置上下文参数 kubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=kubelet-bootstrap.kubeconfig # 设置默认上下文 kubectl config use-context default --kubeconfig=kubelet-bootstrap.kubeconfig cp kubelet-bootstrap.kubeconfig /etc/kubernetes/ scp kubelet-bootstrap.kubeconfig 10.200.46.40:/etc/kubernetes/ scp kubelet-bootstrap.kubeconfig 10.</description>
    </item>
    
    <item>
      <title>K8s 二进制部署6.3（部署 kube-proxy 组件）</title>
      <link>https://SomethingCM.github.io/k8s/k8s6.3/</link>
      <pubDate>Wed, 10 Oct 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/k8s6.3/</guid>
      <description>1、创建 kube-proxy 证书 kube-proxy 运行在所有 worker 节点上，它监听 apiserver 中 service 和 endpoint 的变化情况，创建路由规则以提供服务 IP 和负载均衡功能。 各节点需要安装 ipvsadm 和 ipset 命令，加载 ip_vs 内核模块。 yum install -y epel-release yum install -y conntrack ipvsadm ntp ntpdate ipset jq iptables curl sysstat libseccomp &amp;amp;&amp;amp; modprobe ip_vs 在证书节点上操作 cat &amp;gt; kube-proxy-csr.json &amp;lt;&amp;lt;EOF { &amp;quot;CN&amp;quot;: &amp;quot;system:kube-proxy&amp;quot;, &amp;quot;key&amp;quot;: { &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;, &amp;quot;size&amp;quot;: 2048 }, &amp;quot;names&amp;quot;: [ { &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;, &amp;quot;ST&amp;quot;: &amp;quot;BeiJing&amp;quot;, &amp;quot;L&amp;quot;: &amp;quot;BeiJing&amp;quot;, &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;, &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot; } ] } EOF # CN：指定该证书的 User 为 system:kube-proxy； # 预定义的 RoleBinding system:node-proxier 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限； # 该证书只会被 kube-proxy 当做 client 证书使用，所以 hosts 字段为空； cfssl gencert -ca=ca.</description>
    </item>
    
    <item>
      <title>K8s 二进制部署6（部署worker节点）</title>
      <link>https://SomethingCM.github.io/k8s/k8s6/</link>
      <pubDate>Wed, 10 Oct 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/k8s6/</guid>
      <description>kubernetes worker 节点运行如下组件： * docker(初始化的时候部署过了) * kubelet * kube-proxy * calico/flanneld Node 节点 wget https://storage.googleapis.com/kubernetes-release/release/v1.15.0/kubernetes-node-linux-amd64.tar.gz tar -zxf kubernetes-node-linux-amd64.tar.gz cp kubernetes/node/bin/* /usr/bin/  </description>
    </item>
    
    <item>
      <title>K8s 二进制部署5.1（master节点部署apiserver）</title>
      <link>https://SomethingCM.github.io/k8s/k8s5.1/</link>
      <pubDate>Tue, 02 Oct 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/k8s5.1/</guid>
      <description>1、创建apiserver证书 cd /root/cert cat &amp;gt; kubernetes-csr.json &amp;lt;&amp;lt;EOF { &amp;quot;CN&amp;quot;: &amp;quot;kubernetes&amp;quot;, &amp;quot;hosts&amp;quot;: [ &amp;quot;127.0.0.1&amp;quot;, &amp;quot;10.200.46.44&amp;quot;, &amp;quot;10.200.46.39&amp;quot;, &amp;quot;10.200.46.40&amp;quot;, &amp;quot;10.200.46.41&amp;quot;, &amp;quot;10.68.0.1&amp;quot;, &amp;quot;kubernetes&amp;quot;, &amp;quot;kubernetes.default&amp;quot;, &amp;quot;kubernetes.default.svc&amp;quot;, &amp;quot;kubernetes.default.svc.cluster&amp;quot;, &amp;quot;kubernetes.default.svc.cluster.local&amp;quot; ], &amp;quot;key&amp;quot;: { &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;, &amp;quot;size&amp;quot;: 2048 }, &amp;quot;names&amp;quot;: [ { &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;, &amp;quot;ST&amp;quot;: &amp;quot;Beijing&amp;quot;, &amp;quot;L&amp;quot;: &amp;quot;XS&amp;quot;, &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;, &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot; } ] } EOF cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes cp kubernetes*.pem /etc/kubernetes/pki/ scp kubernetes*.pem 10.200.46.40:/etc/kubernetes/pki/ scp kubernetes*.pem 10.200.46.41:/etc/kubernetes/pki/ # hosts 字段指定授权使用该证书的 IP 和域名列表，这里列出了 master 节点 IP、kubernetes 服务的 IP 和域名； # kubernetes 服务 IP 是 apiserver 自动创建的，一般是 --service-cluster-ip-range 参数指定的网段的第一个IP，后续可以通过下面命令获取： kubectl get svc kubernetes  2、创建加密配置文件 创建加密配置文件 cat &amp;gt; /etc/kubernetes/encryption-config.</description>
    </item>
    
    <item>
      <title>K8s 二进制部署5.2（master节点部署kube-controller-manager）</title>
      <link>https://SomethingCM.github.io/k8s/k8s5.2/</link>
      <pubDate>Tue, 02 Oct 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/k8s5.2/</guid>
      <description>1、创建 kube-controller-manager 证书和私钥 该集群包含 3 个节点，启动后将通过竞争选举机制产生一个 leader 节点，其它节点为阻塞状态。当 leader 节点不可用时，阻塞的节点将再次进行选举产生新的 leader 节点，从而保证服务的可用性。 为保证通信安全，本文档先生成 x509 证书和私钥，kube-controller-manager 在如下两种情况下使用该证书： 1. 与 kube-apiserver 的安全端口通信; 2. 在安全端口(https，10252) 输出 prometheus 格式的 metrics； 创建证书签名请求： cat &amp;gt; kube-controller-manager-csr.json &amp;lt;&amp;lt;EOF { &amp;quot;CN&amp;quot;: &amp;quot;system:kube-controller-manager&amp;quot;, &amp;quot;key&amp;quot;: { &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;, &amp;quot;size&amp;quot;: 2048 }, &amp;quot;hosts&amp;quot;: [ &amp;quot;127.0.0.1&amp;quot;, &amp;quot;10.200.46.44&amp;quot;, &amp;quot;10.200.46.39&amp;quot;, &amp;quot;10.200.46.40&amp;quot;, &amp;quot;10.200.46.41&amp;quot; ], &amp;quot;names&amp;quot;: [ { &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;, &amp;quot;ST&amp;quot;: &amp;quot;BeiJing&amp;quot;, &amp;quot;L&amp;quot;: &amp;quot;BeiJing&amp;quot;, &amp;quot;O&amp;quot;: &amp;quot;system:kube-controller-manager&amp;quot;, &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot; } ] } EOF # hosts 列表包含所有 kube-controller-manager 节点 IP； # CN 和 O 均为 system:kube-controller-manager，kubernetes 内置的 ClusterRoleBindings system:kube-controller-manager 赋予 kube-controller-manager 工作所需的权限。 cfssl gencert -ca=ca.</description>
    </item>
    
    <item>
      <title>K8s 二进制部署5.3（master节点部署kube-schedule）</title>
      <link>https://SomethingCM.github.io/k8s/k8s5.3/</link>
      <pubDate>Tue, 02 Oct 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/k8s5.3/</guid>
      <description>1、创建 kube-scheduler 证书和私钥 该集群包含 3 个节点，启动后将通过竞争选举机制产生一个 leader 节点，其它节点为阻塞状态。当 leader 节点不可用后，剩余节点将再次进行选举产生新的 leader 节点，从而保证服务的可用性。 为保证通信安全，本文档先生成 x509 证书和私钥，kube-scheduler 在如下两种情况下使用该证书： 1. 与 kube-apiserver 的安全端口通信; 2. 在安全端口(https，10251) 输出 prometheus 格式的 metrics； 创建 kube-scheduler 证书和私钥 cat &amp;gt; kube-scheduler-csr.json &amp;lt;&amp;lt;EOF { &amp;quot;CN&amp;quot;: &amp;quot;system:kube-scheduler&amp;quot;, &amp;quot;hosts&amp;quot;: [ &amp;quot;127.0.0.1&amp;quot;, &amp;quot;10.200.46.44&amp;quot;, &amp;quot;10.200.46.39&amp;quot;, &amp;quot;10.200.46.40&amp;quot;, &amp;quot;10.200.46.41&amp;quot; ], &amp;quot;key&amp;quot;: { &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;, &amp;quot;size&amp;quot;: 2048 }, &amp;quot;names&amp;quot;: [ { &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;, &amp;quot;ST&amp;quot;: &amp;quot;BeiJing&amp;quot;, &amp;quot;L&amp;quot;: &amp;quot;BeiJing&amp;quot;, &amp;quot;O&amp;quot;: &amp;quot;system:kube-scheduler&amp;quot;, &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot; } ] } EOF # hosts 列表包含所有 kube-scheduler 节点 IP； # CN 和 O 均为 system:kube-scheduler，kubernetes 内置的 ClusterRoleBindings system:kube-scheduler 将赋予 kube-scheduler 工作所需的权限 cfssl gencert -ca=ca.</description>
    </item>
    
    <item>
      <title>K8s 二进制部署5（部署master节点）</title>
      <link>https://SomethingCM.github.io/k8s/k8s5/</link>
      <pubDate>Tue, 02 Oct 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/k8s5/</guid>
      <description>kubernetes master 节点运行如下组件： # kube-apiserver # kube-scheduler # kube-controller-manager kube-apiserver、kube-scheduler 和 kube-controller-manager 均以多实例模式运行： 1. kube-scheduler 和 kube-controller-manager 会自动选举产生一个 leader 实例，其它实例处于阻塞模式，当 leader 挂了后，重新选举产生新的 leader，从而保证服务可用性； 2. kube-apiserver 是无状态的，需要通过 HAProxy 进行代理访问，从而保证服务可用性； 部署相关二进制文件 server端下载： wget https://storage.googleapis.com/kubernetes-release/release/v1.15.0/kubernetes-server-linux-amd64.tar.gz tar -zxf kubernetes-node-linux-amd64.tar.gz cp kubernetes/server/bin/{apiextensions-apiserver,cloud-controller-manager,hyperkube,kube-apiserver,kube-controller-manager,kube-scheduler,kubelet,kubectl,kube-proxy,mounter} /usr/bin/  </description>
    </item>
    
    <item>
      <title>K8s 二进制部署4（部署 kubectl）</title>
      <link>https://SomethingCM.github.io/k8s/k8s4/</link>
      <pubDate>Mon, 01 Oct 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/k8s4/</guid>
      <description>1、部署 kubectl 命令行工具 下载和分发 kubectl 二进制文件 部署相关二进制文件 server端下载： wget https://storage.googleapis.com/kubernetes-release/release/v1.15.0/kubernetes-server-linux-amd64.tar.gz tar -zxf kubernetes-node-linux-amd64.tar.gz cp kubernetes/server/bin/{apiextensions-apiserver,cloud-controller-manager,hyperkube,kube-apiserver,kube-controller-manager,kube-scheduler,kubelet,kubectl,kube-proxy,mounter} /usr/bin/ 本次要用的是kubelet  2、创建 admin 证书和私钥 cat admin-csr.json { &amp;quot;CN&amp;quot;: &amp;quot;admin&amp;quot;, &amp;quot;hosts&amp;quot;: [], &amp;quot;key&amp;quot;: { &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;, &amp;quot;size&amp;quot;: 2048 }, &amp;quot;names&amp;quot;: [ { &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;, &amp;quot;ST&amp;quot;: &amp;quot;Beijing&amp;quot;, &amp;quot;L&amp;quot;: &amp;quot;XS&amp;quot;, &amp;quot;O&amp;quot;: &amp;quot;system:masters&amp;quot;, &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot; } ] } cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin cp admin* /etc/kubernetes/pki/ scp admin* 10.200.46.40:/etc/kubernetes/pki/ scp admin* 10.</description>
    </item>
    
    <item>
      <title>K8s 二进制部署2（证书创建）</title>
      <link>https://SomethingCM.github.io/k8s/k8s2/</link>
      <pubDate>Wed, 12 Sep 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/k8s2/</guid>
      <description>1、安装cfssl 安装cfssl， CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件 wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 chmod +x cfssl_linux-amd64 mv cfssl_linux-amd64 /usr/local/bin/cfssl wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x cfssljson_linux-amd64 mv cfssljson_linux-amd64 /usr/local/bin/cfssljson wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 chmod +x cfssl-certinfo_linux-amd64 mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo 为了保证通信安全，客户端(如 etcdctl) 与 etcd 集群、etcd 集群之间的通信需要使用 TLS 加密，本节创建 etcd TLS 加密所需的证书和私钥。  2、创建 CA 配置文件 生产ca 根证书 mkdir cert cd cert cat &amp;gt; ca-config.json &amp;lt;&amp;lt;EOF { &amp;quot;signing&amp;quot;: { &amp;quot;default&amp;quot;: { &amp;quot;expiry&amp;quot;: &amp;quot;876000h&amp;quot; }, &amp;quot;profiles&amp;quot;: { &amp;quot;kubernetes&amp;quot;: { &amp;quot;usages&amp;quot;: [ &amp;quot;signing&amp;quot;, &amp;quot;key encipherment&amp;quot;, &amp;quot;server auth&amp;quot;, &amp;quot;client auth&amp;quot; ], &amp;quot;expiry&amp;quot;: &amp;quot;87600h&amp;quot; } } } } EOF # ca-config.</description>
    </item>
    
    <item>
      <title>K8s 二进制部署3（部署-etcd）</title>
      <link>https://SomethingCM.github.io/k8s/k8s3/</link>
      <pubDate>Mon, 10 Sep 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/k8s3/</guid>
      <description>1、创建Etcd相关证书 mkdir etcd cd etcd cat &amp;gt; etcd-csr.json &amp;lt;&amp;lt;EOF { &amp;quot;CN&amp;quot;: &amp;quot;etcd&amp;quot;, &amp;quot;hosts&amp;quot;: [ &amp;quot;127.0.0.1&amp;quot;, &amp;quot;10.200.46.44&amp;quot;, &amp;quot;10.200.46.39&amp;quot;, &amp;quot;10.200.46.40&amp;quot;, &amp;quot;10.200.46.41&amp;quot; ], &amp;quot;key&amp;quot;: { &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;, &amp;quot;size&amp;quot;: 2048 }, &amp;quot;names&amp;quot;: [ { &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;, &amp;quot;ST&amp;quot;: &amp;quot;BeiJing&amp;quot;, &amp;quot;L&amp;quot;: &amp;quot;BeiJing&amp;quot;, &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;, &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot; } ] } EOF # hosts 字段指定授权使用该证书的 etcd 节点 IP； # 每个节点IP 都要在里面 或者 每个机器申请一个对应IP的证书 cfssl gencert -ca=../ca.pem -ca-key=../ca-key.pem -config=../ca-config.json -profile=kubernetes etcd-csr.json | cfssljson -bare etcd 新建k8s 证书目录 pwd /root/cert mkdir -p /etc/kubernetes/pki/etcd/ 拷贝ca证书到/etc/kubernetes/pki cp ca-key.</description>
    </item>
    
    <item>
      <title>K8s Harbor集群</title>
      <link>https://SomethingCM.github.io/k8s/harbor1/</link>
      <pubDate>Sat, 08 Sep 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/harbor1/</guid>
      <description>环境： master1 master2 master3
挂在NAS存储
 postgresql 主备安装 redis sentinel 安装及设置vip略 harbor配置 slb 设置 vip 解析域名 测试  1、NAS挂载 安装节点： master2 master3
yum install nfs-utils -y mkdir /data/registry vim /etc/fstab xxxxxxx.cn-shanghai.nas.aliyuncs.com:/ /data/registry nfs4 vers=4.0,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,_netdev,noresvport 0 0 mount -a  2、postgresql 主备安装 安装节点： master2 master3
yum install -y https://download.postgresql.org/pub/repos/yum/9.6/redhat/rhel-7-x86_64/pgdg-centos96-9.6-3.noarch.rpm yum install -y postgresql96-server postgresql96  执行初始化：
su - root /usr/pgsql-9.6/bin/postgresql96-setup initdb systemctl start postgresql-9.6 &amp;amp;&amp;amp; systemctl enable postgresql-9.6 设置postgres密码 su - postgres psql ALTER USER postgres WITH encrypted PASSWORD &#39;xxxx&#39;; CREATE ROLE replica login replication encrypted password &#39;replica&#39;;  2.</description>
    </item>
    
    <item>
      <title>K8s 二进制部署1（初始化）</title>
      <link>https://SomethingCM.github.io/k8s/k8s1/</link>
      <pubDate>Sat, 08 Sep 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/k8s1/</guid>
      <description>1、系统环境 vip 10.200.46.44 (keepalived haproxy) k8s-1 10.200.46.39 k8s-3 10.200.46.40 k8s-2 10.200.46.41 k8s-4 10.200.46.43 k8s-5 10.200.46.42 # 所有主机：基本系统配置 centos7.3  1.1 无密码登录（看自己需求） ssh-keygen for i in 40 41 42 43 ;do ssh-copy-id 10.200.46.$i;done # 关闭Selinux/firewalld systemctl stop firewalld systemctl disable firewalld setenforce 0 sed -i &amp;quot;s/SELINUX=enforcing/SELINUX=disabled/g&amp;quot; /etc/selinux/config  1.2、关闭交换分区 swapoff -a yes | cp /etc/fstab /etc/fstab_bak cat /etc/fstab_bak |grep -v swap &amp;gt; /etc/fstab  1.3、同步时间 yum install -y ntpdate ntpdate -u ntp.</description>
    </item>
    
    <item>
      <title>K8s kubeadm快速部署</title>
      <link>https://SomethingCM.github.io/k8s/install_1/</link>
      <pubDate>Wed, 18 Apr 2018 01:19:14 +0800</pubDate>
      
      <guid>https://SomethingCM.github.io/k8s/install_1/</guid>
      <description>1、系统环境 vip 10.100.48.10 (keepalived haproxy) k8s-1 10.100.48.87 k8s-2 10.100.48.237 k8s-3 10.100.48.48 k8s-4 10.100.48.113 k8s-5 10.100.48.72 k8s-6 10.100.48.228 # 所有主机：基本系统配置 centos7.3  1.1 无密码登录（看自己需求） ssh-keygen for i in 237 48 113 72 228 ;do ssh-copy-id 10.100.48.$i;done # 关闭Selinux/firewalld systemctl stop firewalld systemctl disable firewalld setenforce 0 sed -i &amp;quot;s/SELINUX=enforcing/SELINUX=disabled/g&amp;quot; /etc/selinux/config  1.2、关闭交换分区 swapoff -a yes | cp /etc/fstab /etc/fstab_bak cat /etc/fstab_bak |grep -v swap &amp;gt; /etc/fstab  1.3、同步时间 yum install -y ntpdate ntpdate -u ntp.</description>
    </item>
    
  </channel>
</rss>