<!DOCTYPE html>
<html class="no-js" lang="en-us" xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="description" content="K8s kubeadm快速部署">
    <meta name="author" content="">

    <meta name="keyword" content="">
    <link rel="shortcut icon" href="/favicon.ico">

    <title>K8s kubeadm快速部署 &middot; VCoKaTA</title>

   	
    
        <link rel="stylesheet" href="https://vcokata.github.io/css/theme/flatly.css">
    

    <script defer src="https://use.fontawesome.com/releases/v5.6.3/js/solid.js" integrity="sha384-F4BRNf3onawQt7LDHDJm/hwm3wBtbLIfGk1VSB/3nn3E+7Rox1YpYcKJMsmHBJIl" crossorigin="anonymous"></script>
    <script defer src="https://use.fontawesome.com/releases/v5.6.3/js/regular.js" integrity="sha384-V+AkgA1cZ+p3DRK63AHCaXvO68V7B5eHoxl7QVN21zftbkFn/sGAIVR7vmQL3Zhp" crossorigin="anonymous"></script>
    <script defer src="https://use.fontawesome.com/releases/v5.6.3/js/brands.js" integrity="sha384-VLgz+MgaFCnsFLiBwE3ItNouuqbWV2ZnIqfsA6QRHksEAQfgbcoaQ4PP0ZeS0zS5" crossorigin="anonymous"></script>
    <script defer src="https://use.fontawesome.com/releases/v5.6.3/js/fontawesome.js" integrity="sha384-treYPdjUrP4rW5q82SnECO7TPVAz4bpas16yuE9F5o7CeBn2YYw1yr5oC8s8Mf8t" crossorigin="anonymous"></script>

   	
   	<link rel="stylesheet" href="https://vcokata.github.io/css/style.css">


    
    <script src="https://vcokata.github.io/js/jquery.min-2.1.4.js"></script>
    <script src="https://vcokata.github.io/js/bootstrap.min-3.3.5.js"></script>
<script src="http://cdn.bootcss.com/highlight.js/9.0.0/highlight.min.js"></script><link href="http://cdn.bootcss.com/highlight.js/9.0.0/styles/default.min.css" rel="stylesheet"><script>hljs.initHighlightingOnLoad();</script>
    
    <link href="" rel="alternate" type="application/rss+xml" title="VCoKaTA" />
</head>
<body lang="en">
    
    <div class="container">
    <div class="row">
        <div class="navbar navbar-default navbar-inverse" role="navigation">
            <div class="navbar-header">
                <a class="navbar-brand" href="https://vcokata.github.io">VCoKaTA</a>
            </div>
            <div class="navbar-collapse collapse navbar-responsive-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li><a href="https://vcokata.github.io/openstack/">Openstack</a></li>
                    <li><a href="https://vcokata.github.io/ceph/">Ceph</a></li>
                    <li><a href="https://vcokata.github.io/k8s/">Kubernetes</a></li>
                    <li><a href="https://vcokata.github.io/mesos/">Mesos</a></li>
                    <li><a href="https://vcokata.github.io/target/">Linux札记</a></li>
                    <li><a href="https://vcokata.github.io/note/">Python &amp; Django</a></li>
                                        <li><a href="https://vcokata.github.io/page/">个人</a></li>
                    
                </ul>
            </div>
        </div>
    </div>
</div>



<div>
    <div class="container col-md-9 ">
        <div class="row">
            <div class="col-md-offset-1 col-md-10">
                     using tags
                    
                        
                        <a href="/tags/%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2">快速部署</a>
                    
                </small>
            </div>
        </div>
        <div class="row">
            <div class="col-md-offset-1 col-md-10">
                

                

<hr />

<h3 id="1-系统环境">1、系统环境</h3>

<pre><code>vip
10.100.48.10 (keepalived haproxy)
k8s-1 10.100.48.87
k8s-2 10.100.48.237
k8s-3 10.100.48.48
k8s-4 10.100.48.113
k8s-5 10.100.48.72
k8s-6 10.100.48.228
# 所有主机：基本系统配置 centos7.3
</code></pre>

<h3 id="1-1-无密码登录-看自己需求">1.1 无密码登录（看自己需求）</h3>

<pre><code>ssh-keygen
 for i in 237 48 113 72 228 ;do ssh-copy-id 10.100.48.$i;done
# 关闭Selinux/firewalld
systemctl stop firewalld
systemctl disable firewalld
setenforce 0
sed -i &quot;s/SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config
</code></pre>

<h3 id="1-2-关闭交换分区">1.2、关闭交换分区</h3>

<pre><code>swapoff -a
yes | cp /etc/fstab /etc/fstab_bak
cat /etc/fstab_bak |grep -v swap &gt; /etc/fstab
</code></pre>

<h3 id="1-3-同步时间">1.3、同步时间</h3>

<pre><code>yum install -y ntpdate
ntpdate -u ntp.api.bz
</code></pre>

<h3 id="1-4-升级内核">1.4、升级内核</h3>

<pre><code>看下内核先
uname -r
3.10.0-327.22.2.el7.x86_64

升级内核需要先导入elrepo的key，然后安装elrepo的yum源
rpm -import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
rpm -Uvh  https://www.elrepo.org/elrepo-release-7.0-4.el7.elrepo.noarch.rpm
#rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm

看下源上的内核版本
 yum --disablerepo=&quot;*&quot; --enablerepo=&quot;elrepo-kernel&quot; list available

安装内核
yum -y --enablerepo=elrepo-kernel install kernel-ml.x86_64 kernel-ml-devel.x86_64

查看内核是否安装完成
rpm -qa|grep kernel
kernel-3.10.0-327.22.2.el7.x86_64
kernel-lt-4.4.157-1.el7.elrepo.x86_64
kernel-tools-libs-3.10.0-327.22.2.el7.x86_64
kernel-tools-3.10.0-327.22.2.el7.x86_64
kernel-lt-devel-4.4.157-1.el7.elrepo.x86_64
kernel-ml-devel-4.18.9-1.el7.elrepo.x86_64
kernel-ml-4.18.9-1.el7.elrepo.x86_64

查看并修改grub的启动顺序
awk -F\' '$1==&quot;menuentry &quot; {print $2}' /etc/grub2.cfg
CentOS Linux (4.18.9-1.el7.elrepo.x86_64) 7 (Core)
CentOS Linux (4.4.157-1.el7.elrepo.x86_64) 7 (Core)
CentOS Linux (3.10.0-327.22.2.el7.x86_64) 7 (Core)
CentOS Linux (0-rescue-4b7b1e6662fa4773a1e9b431c65e98f8) 7 (Core)
注：第一个是4.18，但是对应的顺序是0
vi /etc/default/grub
GRUB_DISTRIBUTOR=&quot;$(sed 's, release .*$,,g' /etc/system-release)&quot;
GRUB_DEFAULT=0
GRUB_DISABLE_SUBMENU=true
GRUB_TERMINAL_OUTPUT=&quot;console&quot;
GRUB_CMDLINE_LINUX=&quot;console=tty0 console=ttyS0,115200 crashkernel=auto rhgb quiet&quot;
GRUB_DISABLE_RECOVERY=“true&quot;

生成grub重新创建内核配置并重启
 grub2-mkconfig -o /boot/grub2/grub.cfg
Generating grub configuration file ...
Found linux image: /boot/vmlinuz-4.18.9-1.el7.elrepo.x86_64
Found initrd image: /boot/initramfs-4.18.9-1.el7.elrepo.x86_64.img
Found linux image: /boot/vmlinuz-4.4.157-1.el7.elrepo.x86_64
Found initrd image: /boot/initramfs-4.4.157-1.el7.elrepo.x86_64.img
Found linux image: /boot/vmlinuz-3.10.0-327.22.2.el7.x86_64
Found initrd image: /boot/initramfs-3.10.0-327.22.2.el7.x86_64.img
Found linux image: /boot/vmlinuz-0-rescue-4b7b1e6662fa4773a1e9b431c65e98f8
Found initrd image: /boot/initramfs-0-rescue-4b7b1e6662fa4773a1e9b431c65e98f8.img
done

reboot
uname  -r
4.18.9-1.el7.elrepo.x86_64

# 设置网桥包经IPTables，core文件生成路径
echo &quot;&quot;&quot;
vm.swappiness = 0
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
&quot;&quot;&quot; &gt; /etc/sysctl.conf
sysctl -p

# 确认内核高于4.1后，开启IPVS
uname -a

cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF
#!/bin/bash
ipvs_modules=&quot;ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr   ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo   ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack&quot;
for kernel_module in \${ipvs_modules}; do
    /sbin/modinfo -F filename \${kernel_module} &gt; /dev/null 2&gt;&amp;1
    if [ $? -eq 0 ]; then
        /sbin/modprobe \${kernel_module}
    fi
done
EOF
chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs
执行sysctl -p报错可执行modprobe br_netfilter，请参考centos7添加bridge-nf-call-ip6tables出现No such file or directory


所有机器需要设定/etc/sysctl.d/k8s.conf的系统参数

# ipvsadm -l --timout
# 修复ipvs模式下长连接timeout问题 小于900即可
cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_intvl = 30
net.ipv4.tcp_keepalive_probes = 10
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
net.ipv4.neigh.default.gc_stale_time = 120
net.ipv4.conf.all.rp_filter = 0
net.ipv4.conf.default.rp_filter = 0
net.ipv4.conf.default.arp_announce = 2
net.ipv4.conf.lo.arp_announce = 2
net.ipv4.conf.all.arp_announce = 2
net.ipv4.ip_forward = 1
net.ipv4.tcp_max_tw_buckets = 5000
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 1024
net.ipv4.tcp_synack_retries = 2
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.netfilter.nf_conntrack_max = 2310720
fs.inotify.max_user_watches=89100
fs.may_detach_mounts = 1
fs.file-max = 52706963
fs.nr_open = 52706963
net.bridge.bridge-nf-call-arptables = 1
vm.swappiness = 0
vm.overcommit_memory=1
vm.panic_on_oom=0
EOF
sysctl --system
</code></pre>

<h2 id="2-安装软件docker">2、安装软件docker</h2>

<pre><code># 安装docker
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo

yum install -y yum-utils
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum makecache fast

## 列出Docker版本
yum list docker-ce --showduplicates | sort -r

## 安装指定版本
sudo yum install docker-ce-&lt;VERSION_STRING&gt;
echo &quot;{
 &quot;storage-driver&quot;: &quot;overlay2&quot;,
 &quot;storage-opts&quot;: [ &quot;overlay2.override_kernel_check=true&quot; ],
 &quot;live-restore&quot; : false
  }&quot; &gt;&gt; /etc/docker/daemon.json

  # 启动docker
  sed -i &quot;13i ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT&quot; /usr/lib/systemd/system/docker.service

  systemctl daemon-reload
  systemctl enable docker
  systemctl start docker

mkfs.xfs -n ftype=1 /dev/sdx
官方指定xfs格式磁盘必须使用ftype=1，overlay驱动才能识别
</code></pre>

<p><img style="zoom:70%" src="/images/k8s/docker01.png"></p>

<h2 id="3-安装keepalived-haproxy">3、安装keepalived、haproxy</h2>

<pre><code># 安装文件管理器，XShell可通过rz sz命令上传或者下载服务器文件
yum install lrzsz -y

yum install -y socat keepalived ipvsadm haproxy
</code></pre>

<h3 id="3-1-keepalive-配置">3.1、Keepalive 配置</h3>

<pre><code>k8s-1
cat &gt;/etc/keepalived/keepalived.conf  &lt;&lt;EOL
global_defs {
   notification_email {
      chenmeng@yunzongnet.com
   }
}
vrrp_instance VI_1 {
    state MASTER
    nopreempt
    interface eth0
    virtual_router_id 121
    priority 60
    lvs_sync_daemon_interface eth0
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass adminyz
    }
    virtual_ipaddress {
        10.100.48.10/32 dev eth0
    }
}
EOL
k8s-2
cat &gt;/etc/keepalived/keepalived.conf  &lt;&lt;EOL
global_defs {
   notification_email {
      chenmeng@yunzongnet.com
  }
}
vrrp_instance VI_1 {
    state BACKUP
    nopreempt
    interface eth0
    virtual_router_id 121
    priority 60
    lvs_sync_daemon_interface eth0
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass adminyz
    }
    virtual_ipaddress {
        10.100.48.10/32 dev eth0
    }
}
EOL
k8s-3
cat &gt;/etc/keepalived/keepalived.conf  &lt;&lt;EOL
global_defs {
   notification_email {
      chenmeng@yunzongnet.com
  }
}
vrrp_instance VI_1 {
    state BACKUP
    nopreempt
    interface eth0
    virtual_router_id 121
    priority 60
    lvs_sync_daemon_interface eth0
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass adminyz
    }
    virtual_ipaddress {
        10.100.48.10/32 dev eth0
    }
}
EOL
echo net.ipv4.ip_nonlocal_bind = 1 &gt;&gt;/etc/sysctl.conf
sysctl -p
systemctl enable keepalived
systemctl start keepalived
</code></pre>

<h3 id="3-2-haproxy-配置">3.2、haproxy 配置</h3>

<pre><code>cat /etc/haproxy/haproxy.cfg
global
    log         127.0.0.1 local2
    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon
    stats socket /var/lib/haproxy/stats
defaults
    mode                    tcp
    log                     global
    option                  tcplog
    option                  dontlognull
    option                  redispatch
    retries                 3
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout check           10s
    maxconn                 3000
listen stats
    mode   http
    bind :10086
    stats   enable
    stats   uri     /admin?stats
    stats   auth    admin:admin
    stats   admin   if TRUE

listen k8s_etcd
    bind 10.100.48.10:2379
    balance  source
    option  tcpka
    option  httpchk
    option  tcplog
    server k8s-1 10.100.48.87:2379  check inter 10000 fall 2 rise 2 weight 1
    server k8s-2 10.100.48.237:2379 check inter 10000 fall 2 rise 2 weight 1
    server k8s-3 10.100.48.48:2379  check inter 10000 fall 2 rise 2 weight 1
listen k8s_https
    bind 10.100.48.10:6443
    mode      tcp
    maxconn      2000
    balance      roundrobin
    server k8s-1 10.100.48.87:6443  check inter 10000 fall 2 rise 2 weight 1
    server k8s-2 10.100.48.237:6443  check inter 10000 fall 2 rise 2 weight 1
    server k8s-3 10.100.48.48:6443  check inter 10000 fall 2 rise 2 weight 1
systemctl enable haproxy
systemctl start haproxy

</code></pre>

<h2 id="4-安装etcd">4、安装etcd</h2>

<pre><code>yum install -y etcd(也可以下载二进制包部署)

K8s-1
vim /usr/lib/systemd/system/etcd.service
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
#EnvironmentFile=-/etc/etcd/etcd.conf
User=etcd
# set GOMAXPROCS to number of processors
ExecStart=/usr/bin/etcd \
  --name=k8s-1 \
  --initial-advertise-peer-urls=http://10.100.48.87:2380 \
  --listen-peer-urls=http://10.100.48.87:2380 \
  --listen-client-urls=http://10.100.48.87:2379,http://127.0.0.1:2379 \
  --advertise-client-urls=http://10.100.48.87:2379 \
  --initial-cluster-token=etcd-cluster-0 \
  --initial-cluster=k8s-1=http://10.100.48.87:2380,k8s-2=http://10.100.48.237:2380,k8s-3=http://10.100.48.48:2380 \
  --initial-cluster-state=new \
  --data-dir=/var/lib/etcd
Restart=on-failure
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target

k8s-2
vim /usr/lib/systemd/system/etcd.service
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
User=etcd
ExecStart=/usr/bin/etcd \
  --name=k8s-2 \
  --initial-advertise-peer-urls=http://10.100.48.237:2380 \
  --listen-peer-urls=http://10.100.48.237:2380 \
  --listen-client-urls=http://10.100.48.237:2379,http://127.0.0.1:2379 \
  --advertise-client-urls=http://10.100.48.237:2379 \
  --initial-cluster-token=etcd-cluster-0 \
  --initial-cluster=k8s-1=http://10.100.48.87:2380,k8s-2=http://10.100.48.237:2380,k8s-3=http://10.100.48.48:2380 \
  --initial-cluster-state=new \
  --data-dir=/var/lib/etcd
Restart=on-failure
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target

K8s-3
cat /usr/lib/systemd/system/etcd.service
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
User=etcd
ExecStart=/usr/bin/etcd \
  --name=k8s-3 \
  --initial-advertise-peer-urls=http://10.100.48.48:2380 \
  --listen-peer-urls=http://10.100.48.48:2380 \
  --listen-client-urls=http://10.100.48.48:2379,http://127.0.0.1:2379 \
  --advertise-client-urls=http://10.100.48.48:2379 \
  --initial-cluster-token=etcd-cluster-0 \
  --initial-cluster=k8s-1=http://10.100.48.87:2380,k8s-2=http://10.100.48.237:2380,k8s-3=http://10.100.48.48:2380 \
  --initial-cluster-state=new \
  --data-dir=/var/lib/etcd
Restart=on-failure
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target

systemctl start etcd
ETCDCTL_API=3 etcdctl member list
6763a1217e5c1224, started, k8s-2, http://10.100.48.237:2380, http://10.100.48.237:2379
a4fb67a328b58a7b, started, k8s-1, http://10.100.48.87:2380, http://10.100.48.87:2379
f0282e3a990cacb8, started, k8s-3, http://10.100.48.48:2380, http://10.100.48.48:2379

ETCDCTL_API=3  etcdctl   --endpoints 10.100.48.87:2379,10.100.48.48:2379,10.100.48.237:2379 endpoint status  --write-out=&quot;table&quot;                 
+--------------------+------------------+---------+---------+-----------+-----------+------------+
|      ENDPOINT      |        ID        | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |
+--------------------+------------------+---------+---------+-----------+-----------+------------+
|  10.100.48.87:2379 | a4fb67a328b58a7b |  3.3.11 |   20 kB |      true |         9 |          8 |
|  10.100.48.48:2379 | f0282e3a990cacb8 |  3.3.11 |   20 kB |     false |         9 |          8 |
| 10.100.48.237:2379 | 6763a1217e5c1224 |  3.3.11 |   20 kB |     false |         9 |          8 |
+--------------------+------------------+---------+---------+-----------+-----------+——————+
</code></pre>

<h2 id="5-安装kubernetes相关组件">5、安装kubernetes相关组件</h2>

<h3 id="5-1-配置源">5.1、配置源</h3>

<pre><code>cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
        http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
</code></pre>

<h3 id="5-2-安装配置相关软件">5.2、安装配置相关软件</h3>

<pre><code>yum install -y kubelet kubeadm kubectl ebtables

vim /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf
Environment=&quot;KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs&quot;
# 设置kubelet开机启动
systemctl enable kubelet
systemctl start kubelet
</code></pre>

<h2 id="6-kubeadm创建集群">6、kubeadm创建集群</h2>

<h3 id="6-1-初始化集群">6.1、初始化集群</h3>

<pre><code>kubeadm config print init-defaults

K8s-1

cat  kubeadm-config.yaml    
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: 1.15.0
controlPlaneEndpoint: &quot;10.100.48.10:6443&quot;
etcd:
  external:
    endpoints:
    - https://10.100.48.87:2379
    - https://10.100.48.237:2379
    - https://10.100.48.48:2379
maxPods: 100
networkPlugin: cni
imageRepository: registry.aliyuncs.com/google_containers
apiServer:
  certSANs:
  - k8s-1
  - k8s-2
  - k8s-3
  - 10.100.48.87
  - 10.100.48.237
  - 10.100.48.48
  - 10.100.48.10
networking:
  # This CIDR is a Calico default. Substitute or remove for your CNI provider.
  podSubnet: 10.244.0.0/16
---
apiVersion: kubeproxy.config.k8s.io/v1alpha2
kind: KubeProxyConfiguration
mode: ipvs

kubeadm init --config kubeadm-config.yaml
        mkdir -p $HOME/.kube
        sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
        sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>

<h3 id="6-2-master2-3加入集群">6.2、master2、3加入集群</h3>

<pre><code>K8s-2 3
scp  -dpr k8s-1:/etc/kubernetes/pki/ /etc/kubernetes/
 kubeadm join 10.100.48.10:8443 --token etak2z.tksn3ix4m02xghp1 \
--discovery-token-ca-cert-hash sha256:4265c5bd114a9b24c8bab56c82821cda25e64746d695d46cfea292eee8431841 \    --experimental-control-plane          
        mkdir -p $HOME/.kube
        sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
        sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>

<h3 id="6-3-slave加入集群">6.3、slave加入集群</h3>

<pre><code>安装slave
K8s-4 5 6
kubeadm join 10.100.48.10:8443 --token etak2z.tksn3ix4m02xghp1 \
--discovery-token-ca-cert-hash sha256:4265c5bd114a9b24c8bab56c82821cda25e64746d695d46cfea292eee8431841
</code></pre>

<h2 id="7-安装网络flannel">7、安装网络flannel</h2>

<pre><code>kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/62e44c867a2846fefb68bd5f178daf4da3095ccb/Documentation/kube-flannel.yml

查看状态
[root@k8s-1 ~]# kubectl get cs  
NAME                 STATUS    MESSAGE             ERROR
controller-manager   Healthy   ok                  
scheduler            Healthy   ok                  
etcd-2               Healthy   {&quot;health&quot;:&quot;true&quot;}   
etcd-0               Healthy   {&quot;health&quot;:&quot;true&quot;}   
etcd-1               Healthy   {&quot;health&quot;:&quot;true&quot;}   
[root@k8s-1 ~]# kubectl get node
NAME    STATUS   ROLES    AGE     VERSION
k8s-1   Ready    master   83m     v1.15.0
k8s-2   Ready    master   19m     v1.15.0
k8s-3   Ready    master   10m     v1.15.0
k8s-4   Ready    &lt;none&gt;   2m42s   v1.15.0
k8s-5   Ready    &lt;none&gt;   2m29s   v1.15.0
k8s-6   Ready    &lt;none&gt;   2m14s   v1.15.0
[root@k8s-1 ~]# kubectl get po --all-namespaces
NAMESPACE     NAME                            READY   STATUS    RESTARTS   AGE
kube-system   coredns-bccdc95cf-6gmsx         1/1     Running   0          83m
kube-system   coredns-bccdc95cf-dm9s4         1/1     Running   0          83m
kube-system   kube-apiserver-k8s-1            1/1     Running   0          83m
kube-system   kube-apiserver-k8s-2            1/1     Running   0          19m
kube-system   kube-apiserver-k8s-3            1/1     Running   0          10m
kube-system   kube-controller-manager-k8s-1   1/1     Running   0          83m
kube-system   kube-controller-manager-k8s-2   1/1     Running   0          19m
kube-system   kube-controller-manager-k8s-3   1/1     Running   0          10m
kube-system   kube-flannel-ds-amd64-9jvtc     1/1     Running   0          2m45s
kube-system   kube-flannel-ds-amd64-fkdvm     1/1     Running   0          6m44s
kube-system   kube-flannel-ds-amd64-ldgvl     1/1     Running   0          6m44s
kube-system   kube-flannel-ds-amd64-tt6zr     1/1     Running   1          2m32s
kube-system   kube-flannel-ds-amd64-w9jqd     1/1     Running   0          2m17s
kube-system   kube-flannel-ds-amd64-wgzxb     1/1     Running   0          6m44s
kube-system   kube-proxy-5qcq2                1/1     Running   0          2m32s
kube-system   kube-proxy-bm5bs                1/1     Running   0          19m
kube-system   kube-proxy-pft7f                1/1     Running   0          10m
kube-system   kube-proxy-s7h9k                1/1     Running   0          83m
kube-system   kube-proxy-t5t8b                1/1     Running   0          2m45s
kube-system   kube-proxy-wdvm4                1/1     Running   0          2m17s
kube-system   kube-scheduler-k8s-1            1/1     Running   0          82m
kube-system   kube-scheduler-k8s-2            1/1     Running   0          19m
kube-system   kube-scheduler-k8s-3            1/1     Running   0          10m
</code></pre>

<h2 id="8-安装dashboard">8、安装dashboard</h2>

<pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended/kubernetes-dashboard.yaml
cat kubernetes-dashboard.yaml
# Copyright 2017 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ------------------- Dashboard Secrets ------------------- #
apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-certs
  namespace: kube-system
type: Opaque
---
apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-csrf
  namespace: kube-system
type: Opaque
data:
  csrf: &quot;&quot;
---
# ------------------- Dashboard Service Account ------------------- #
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
---
# ------------------- Dashboard Role &amp; Role Binding ------------------- #
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: kubernetes-dashboard-minimal
  namespace: kube-system
rules:
  # Allow Dashboard to create 'kubernetes-dashboard-key-holder' secret.
- apiGroups: [&quot;&quot;]
  resources: [&quot;secrets&quot;]
  verbs: [&quot;create&quot;]
  # Allow Dashboard to create 'kubernetes-dashboard-settings' config map.
- apiGroups: [&quot;&quot;]
  resources: [&quot;configmaps&quot;]
  verbs: [&quot;create&quot;]
  # Allow Dashboard to get, update and delete Dashboard exclusive secrets.
- apiGroups: [&quot;&quot;]
  resources: [&quot;secrets&quot;]
  resourceNames: [&quot;kubernetes-dashboard-key-holder&quot;, &quot;kubernetes-dashboard-certs&quot;, &quot;kubernetes-dashboard-csrf&quot;]
  verbs: [&quot;get&quot;, &quot;update&quot;, &quot;delete&quot;]
  # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.
- apiGroups: [&quot;&quot;]
  resources: [&quot;configmaps&quot;]
  resourceNames: [&quot;kubernetes-dashboard-settings&quot;]
  verbs: [&quot;get&quot;, &quot;update&quot;]
  # Allow Dashboard to get metrics from heapster.
- apiGroups: [&quot;&quot;]
  resources: [&quot;services&quot;]
  resourceNames: [&quot;heapster&quot;]
  verbs: [&quot;proxy&quot;]
- apiGroups: [&quot;&quot;]
  resources: [&quot;services/proxy&quot;]
  resourceNames: [&quot;heapster&quot;, &quot;http:heapster:&quot;, &quot;https:heapster:&quot;]
  verbs: [&quot;get&quot;]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kubernetes-dashboard-minimal
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubernetes-dashboard-minimal
subjects:
- kind: ServiceAccount
  name: kubernetes-dashboard
  namespace: kube-system
---
# ------------------- Dashboard Deployment ------------------- #
kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: kubernetes-dashboard
  template:
    metadata:
      labels:
        k8s-app: kubernetes-dashboard
    spec:
      containers:
      - name: kubernetes-dashboard
        image: registry.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.10.1
        ports:
        - containerPort: 8443
          protocol: TCP
        args:
          - --auto-generate-certificates
          # Uncomment the following line to manually specify Kubernetes API server Host
          # If not specified, Dashboard will attempt to auto discover the API server and connect
          # to it. Uncomment only if the default does not work.
          # - --apiserver-host=http://my-address:port
        volumeMounts:
        - name: kubernetes-dashboard-certs
          mountPath: /certs
          # Create on-disk volume to store exec logs
        - mountPath: /tmp
          name: tmp-volume
        livenessProbe:
          httpGet:
            scheme: HTTPS
            path: /
            port: 8443
          initialDelaySeconds: 30
          timeoutSeconds: 30
      volumes:
      - name: kubernetes-dashboard-certs
        secret:
          secretName: kubernetes-dashboard-certs
      - name: tmp-volume
        emptyDir: {}
      serviceAccountName: kubernetes-dashboard
      # Comment the following tolerations if Dashboard must not be deployed on master
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
---
# ------------------- Dashboard Service ------------------- #
kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  ports:
    - port: 443
      targetPort: 8443
  selector:
    k8s-app: kubernetes-dashboard
cat dashboard-admin.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kubernetes-dashboard
  labels:
    k8s-app: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: kubernetes-dashboard
  namespace: kube-system
kubectl get secret -n kube-system|grep kubernetes-dashboard
kubernetes-dashboard-certs                       Opaque                                0      17m
kubernetes-dashboard-key-holder                  Opaque                                2      152m
kubernetes-dashboard-token-9sh7p                 kubernetes.io/service-account-token   3      17m
 kubectl describe secret -n kube-system  kubernetes-dashboard-token-9sh7p
Name:         kubernetes-dashboard-token-9sh7p
Namespace:    kube-system
Labels:       &lt;none&gt;
Annotations:  kubernetes.io/service-account.name: kubernetes-dashboard
              kubernetes.io/service-account.uid: 46a215dd-1bd8-4f20-bca1-176297fe096f
Type:  kubernetes.io/service-account-token
Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC10b2tlbi05c2g3cCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjQ2YTIxNWRkLTFiZDgtNGYyMC1iY2ExLTE3NjI5N2ZlMDk2ZiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5ldGVzLWRhc2hib2FyZCJ9.yPCfExLYihstnMciS5N4Qk8IN7xpItnh6uPIe188qYX7zc4OX69cJI0MdQMXyMslJ2_Qkxp27oQScjpo1rFZIEDOIlL2f8GuqTZi3tQNPFBg7haMxCKQBC8jR-gXLyWUrOKObSuGHycCbOdq0HK1HmLbVOAjK3YtKgjtlp_vE0QBTDcBQC-DokTidIquoowWxyMVC9Gj3OK1iIL25nBZFdOMWau1mxOMkq1rdS_3ab_qw6EJMOvHBaTz8pBTC50HEOF-vMq8maPRmOTibOItD86dgy17ORy0b9OmzZl79c-kCdupOj_mV-8glW6zRCe7b0zwoDYhZfzbhkVCTMZpIg
 kubectl --namespace=kube-system get service kubernetes-dashboard
NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE
kubernetes-dashboard   NodePort   10.105.150.53   &lt;none&gt;        443:30787/TCP   26s
登录地址： https://ip:30787
</code></pre>

            </div>
        </div>
        <div class="row">
            <div class="col-md-12">
                <hr>
            </div>
        </div>
        <section id="comments" class="themeform">
<div class="ds-thread" data-thread-key="/k8s/install_1/" data-title="K8s kubeadm快速部署" data-url="https://vcokata.github.io/k8s/install_1/"></div>
<script type="text/javascript">
var duoshuoQuery = {short_name:"Zen"};
    (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] 
         || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
</script>
</section>
    </div>
    <div class="col-md-3 panel panel-success">
         <ul id="tree" class="ztree" style="list-style-type:none"></ul>
    </div>
</div>
<link rel="stylesheet" href="https://vcokata.github.io/css/zTreeStyle.css" type="text/css"><script src="https://vcokata.github.io/js/jquery.ztree.core-3.5.min.js"></script><script src="https://vcokata.github.io/js/ztree_toc.js"></script><script>
$(document).ready(function(){
    $('#tree').ztree_toc({
        is_posion_top:false,
        is_expand_all: true
    });
});
</script>
    <div class="container">
        <div class="row col-md-12">
            <footer>
                <div class="pull-left">
                    <p>
                        &copy;  ~ Powered By <a href="http://hugo.spf13.com">Hugo</a> - version: 0.58.3 ~ <a href="https://vcokata.github.io/license">License</a>
                    </p>
                </div>

                
                <div class="pull-right">
                    
                    
                    
                    
                        <a href="https://github.com/VCoKaTA" target="_blank">
                        <span class="sr-only">GitHub profile</span>
                        <i class="fab fa-github-square fa-2x"></i></a>
                    
                    
                    
                    
                    
                </div>
            </footer>
        </div>
    </div>

    
    </body>
</html>


