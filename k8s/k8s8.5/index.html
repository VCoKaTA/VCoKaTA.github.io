<!DOCTYPE html>
<html class="no-js" lang="en-us" xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="description" content="K8s 二进制部署8.5（部署 Prometheus-operator监控）">
    <meta name="author" content="">

    <meta name="keyword" content="">
    <link rel="shortcut icon" href="/favicon.ico">

    <title>K8s 二进制部署8.5（部署 Prometheus-operator监控） &middot; SomethingCM</title>

   	
    
        <link rel="stylesheet" href="https://SomethingCM.github.io//css/theme/flatly.css">
    

    <script defer src="https://use.fontawesome.com/releases/v5.6.3/js/solid.js" integrity="sha384-F4BRNf3onawQt7LDHDJm/hwm3wBtbLIfGk1VSB/3nn3E+7Rox1YpYcKJMsmHBJIl" crossorigin="anonymous"></script>
    <script defer src="https://use.fontawesome.com/releases/v5.6.3/js/regular.js" integrity="sha384-V+AkgA1cZ+p3DRK63AHCaXvO68V7B5eHoxl7QVN21zftbkFn/sGAIVR7vmQL3Zhp" crossorigin="anonymous"></script>
    <script defer src="https://use.fontawesome.com/releases/v5.6.3/js/brands.js" integrity="sha384-VLgz+MgaFCnsFLiBwE3ItNouuqbWV2ZnIqfsA6QRHksEAQfgbcoaQ4PP0ZeS0zS5" crossorigin="anonymous"></script>
    <script defer src="https://use.fontawesome.com/releases/v5.6.3/js/fontawesome.js" integrity="sha384-treYPdjUrP4rW5q82SnECO7TPVAz4bpas16yuE9F5o7CeBn2YYw1yr5oC8s8Mf8t" crossorigin="anonymous"></script>

   	
   	<link rel="stylesheet" href="https://SomethingCM.github.io//css/style.css">


    
    <script src="https://SomethingCM.github.io//js/jquery.min-2.1.4.js"></script>
    <script src="https://SomethingCM.github.io//js/bootstrap.min-3.3.5.js"></script>
<script src="http://cdn.bootcss.com/highlight.js/9.0.0/highlight.min.js"></script><link href="http://cdn.bootcss.com/highlight.js/9.0.0/styles/default.min.css" rel="stylesheet"><script>hljs.initHighlightingOnLoad();</script>
    
    <link href="" rel="alternate" type="application/rss+xml" title="SomethingCM" />
</head>
<body lang="en">
    
    <div class="container">
    <div class="row">
        <div class="navbar navbar-default navbar-inverse" role="navigation">
            <div class="navbar-header">
                <a class="navbar-brand" href="https://SomethingCM.github.io/">SomethingCM</a>
            </div>
            <div class="navbar-collapse collapse navbar-responsive-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li><a href="https://SomethingCM.github.io//openstack/">Openstack</a></li>
                    <li><a href="https://SomethingCM.github.io//ceph/">Ceph</a></li>
                    <li><a href="https://SomethingCM.github.io//k8s/">Kubernetes</a></li>
                    <li><a href="https://SomethingCM.github.io//mesos/">Mesos</a></li>
                    <li><a href="https://SomethingCM.github.io//target/">Linux札记</a></li>
                    <li><a href="https://SomethingCM.github.io//note/">Python &amp; Django</a></li>
                                        <li><a href="https://SomethingCM.github.io//page/">个人</a></li>
                    
                </ul>
            </div>
        </div>
    </div>
</div>



<div>
    <div class="container col-md-9 ">
        <div class="row">
            <div class="col-md-offset-1 col-md-10">
                    
                        
                        <a href="/categories/%E9%83%A8%E7%BD%B2prometheus-operator%E7%9B%91%E6%8E%A7">部署Prometheus-operator监控</a>
                     using tags
                    
                        
                        <a href="/tags/%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2">二进制部署</a>
                    
                </small>
            </div>
        </div>
        <div class="row">
            <div class="col-md-offset-1 col-md-10">
                

                

<p><font color=#DC143C size=5 face="黑体"> Prometheus Operator不同于Prometheus，Prometheus Operator是 CoreOS 开源的一套用于管理在 Kubernetes 集群上的 Prometheus 控制器，它是为了简化在 Kubernetes 上部署、管理和运行 Prometheus 和 Alertmanager 集群。</font></p>

<p><font color=#DC143C size=5 face="黑体"> 官方提供的架构图</font></p>

<p><img style="zoom:47%" src="/images/k8s/prom01.png"></p>

<pre><code>kubernetes也在官方的github上关于使用prometheus监控的建议：
地址：https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/prometheus
</code></pre>

<h3 id="1-相关服务说明">1、相关服务说明</h3>

<pre><code>　　Operator： Operator 资源会根据自定义资源（Custom Resource Definition / CRDs）来部署和管理 Prometheus Server，同时监控这些自定义资源事件的变化来做相应的处理，是整个系统的控制中心。
　　Prometheus： Prometheus 资源是声明性地描述 Prometheus 部署的期望状态。
　　Prometheus Server： Operator 根据自定义资源 Prometheus 类型中定义的内容而部署的 Prometheus Server 集群，这些自定义资源可以看作是用来管理 Prometheus Server 集群的 StatefulSets 资源。
　　ServiceMonitor： ServiceMonitor 也是一个自定义资源，它描述了一组被 Prometheus 监控的 targets 列表。该资源通过 Labels 来选取对应的 Service Endpoint，让 Prometheus Server 通过选取的 Service 来获取 Metrics 信息。
　　Service： Service 资源主要用来对应 Kubernetes 集群中的 Metrics Server Pod，来提供给 ServiceMonitor 选取让 Prometheus Server 来获取信息。简单的说就是 Prometheus 监控的对象，例如之前了解的 Node Exporter Service、Mysql Exporter Service 等等。
Alertmanager： Alertmanager 也是一个自定义资源类型，由 Operator 根据资源描述内容来部署 Alertmanager 集群

</code></pre>

<h2 id="2-下载部署">2、下载部署</h2>

<h3 id="2-1-下载配置文件">2.1、下载配置文件</h3>

<pre><code>官方地址：https://github.com/coreos/kube-prometheus
因为整个项目并没有多大，这里我把整个项目克隆下来，你也可以下载单独的文件，把https://github.com/coreos/kube-prometheus/tree/master/manifests下面的文件全部下载到本地。
[root@k8s-master01 ~]# git clone https://github.com/coreos/kube-prometheus.git

# 因为官方把所有资源配置文件都放到一个文件目录下，这里我们为了方便，把不同服务的清单文件分别归档
[root@k8s-master01 ~]# cd kube-prometheus/manifests
[root@k8s-1 manifests]# mkdir serviceMonitor operator grafana kube-state-metrics alertmanager node-exporter adapter prometheusalertmanager node-exporter adapter prometheus
[root@k8s-1 manifests]# mv *-serviceMonitor* serviceMonitor/
[root@k8s-1 manifests]# mv 0prometheus-operator* operator/
[root@k8s-1 manifests]# mv grafana-* grafana/
[root@k8s-1 manifests]# mv kube-state-metrics-* kube-state-metrics/
[root@k8s-1 manifests]# mv alertmanager-* alertmanager/
[root@k8s-1 manifests]# mv node-exporter-* node-exporter/
[root@k8s-1 manifests]# mv prometheus-adapter-* adapter/
[root@k8s-1 manifests]# mv prometheus-* prometheus/
</code></pre>

<h3 id="2-2-部署operator">2.2、部署operator</h3>

<pre><code>## 首先创建prometheus监控专有命名空间
[root@k8s-1 manifests]# kubectl apply -f 00namespace-namespace.yaml
## 部署operator
[root@k8s-1 manifests]# kubectl apply -f operator/
## 查看pod运行情况，配置清单中镜像仓库地址为，quay.io,所以无需进行其它操作
[root@k8s-1 manifests]# kubectl get pods -n monitoring
NAME                                  READY   STATUS    RESTARTS   AGE
prometheus-operator-55b978b89-t2wn7   1/1     Running   0          97s

</code></pre>

<h3 id="2-3-部署metrics">2.3、 部署metrics</h3>

<pre><code>这里部署metrics之前，需要先确定集群中kube-apiserver是否已经开启聚合(支持集群接入第三方api)以及其它组件参数是否正确，否则会导致无法获取数据的情况出现.
[root@k8s-1 manifests]# kubectl apply -f kube-state-metrics/
</code></pre>

<h3 id="2-4-部署其它组件">2.4、 部署其它组件</h3>

<pre><code>其它组件按照以上部署即可，镜像无需翻墙均可以正常下载。镜像下载速度取决与本地网络状况。
[root@k8s-1 manifests]# kubectl apply -f adapter/
[root@k8s-1 manifests]# kubectl apply -f alertmanager/
[root@k8s-1 manifests]# kubectl apply -f node-exporter/
[root@k8s-1 manifests]# kubectl apply -f grafana/
[root@k8s-1 manifests]# kubectl apply -f prometheus/
[root@k8s-1 manifests]# kubectl apply -f serviceMonitor/

## 部署完成后，可以查看下个资源运行部署详情
[root@k8s-1 manifests]#  kubectl get all -n monitoring
NAME                                      READY   STATUS    RESTARTS   AGE
pod/alertmanager-main-0                   2/2     Running   0          3m5s
pod/alertmanager-main-1                   2/2     Running   0          3m5s
pod/alertmanager-main-2                   2/2     Running   0          3m5s
pod/grafana-57bfdd47f8-527lr              1/1     Running   0          2m53s
pod/kube-state-metrics-ff5cb7949-qm244    3/3     Running   0          5m42s
pod/node-exporter-7gf4x                   2/2     Running   0          2m59s
pod/node-exporter-hdgd2                   2/2     Running   0          2m59s
pod/node-exporter-j7cjw                   2/2     Running   0          2m59s
pod/node-exporter-kt64c                   2/2     Running   0          2m59s
pod/node-exporter-qkm7k                   2/2     Running   0          2m59s
pod/prometheus-adapter-668748ddbd-6wvzn   1/1     Running   0          3m28s
pod/prometheus-k8s-0                      3/3     Running   1          2m48s
pod/prometheus-k8s-1                      3/3     Running   1          2m48s
pod/prometheus-operator-55b978b89-t2wn7   1/1     Running   0          10m

NAME                            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
service/alertmanager-main       ClusterIP   10.68.128.69   &lt;none&gt;        9093/TCP                     3m6s
service/alertmanager-operated   ClusterIP   None           &lt;none&gt;        9093/TCP,9094/TCP,9094/UDP   3m6s
service/grafana                 ClusterIP   10.68.95.96    &lt;none&gt;        3000/TCP                     2m54s
service/kube-state-metrics      ClusterIP   None           &lt;none&gt;        8443/TCP,9443/TCP            5m43s
service/node-exporter           ClusterIP   None           &lt;none&gt;        9100/TCP                     3m
service/prometheus-adapter      ClusterIP   10.68.9.208    &lt;none&gt;        443/TCP                      3m29s
service/prometheus-k8s          ClusterIP   10.68.82.83    &lt;none&gt;        9090/TCP                     2m49s
service/prometheus-operated     ClusterIP   None           &lt;none&gt;        9090/TCP                     2m50s
service/prometheus-operator     ClusterIP   None           &lt;none&gt;        8080/TCP                     10m
NAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/node-exporter   5         5         5       5            5           kubernetes.io/os=linux   3m1s
NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/grafana               1/1     1            1           2m54s
deployment.apps/kube-state-metrics    1/1     1            1           5m43s
deployment.apps/prometheus-adapter    1/1     1            1           3m29s
deployment.apps/prometheus-operator   1/1     1            1           10m
NAME                                            DESIRED   CURRENT   READY   AGE
replicaset.apps/grafana-57bfdd47f8              1         1         1       2m54s
replicaset.apps/kube-state-metrics-ff5cb7949    1         1         1       5m43s
replicaset.apps/prometheus-adapter-668748ddbd   1         1         1       3m29s
replicaset.apps/prometheus-operator-55b978b89   1         1         1       10m
NAME                                 READY   AGE
statefulset.apps/alertmanager-main   3/3     3m6s
statefulset.apps/prometheus-k8s      2/2     2m50s
</code></pre>

<h2 id="3-创建ingress服务">3、创建ingress服务</h2>

<pre><code>这里我没有采用NodePort的方式暴露服务，而是使用的Ingress，具体Ingress安装部署
# 配置prometheus、grafana、alertmanager三个服务可视化web界面Ingress访问
[root@k8s-1 manifests]# cat ingress-all-svc.yml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: prometheus-ing
  namespace: monitoring
spec:
  rules:
  - host: prometheus.monitoring.k8s.local
    http:
      paths:
      - backend:
          serviceName: prometheus-k8s
          servicePort: 9090
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: grafana-ing
  namespace: monitoring
spec:
  rules:
  - host: grafana.monitoring.k8s.local
    http:
      paths:
      - backend:
          serviceName: grafana
          servicePort: 3000
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: alertmanager-ing
  namespace: monitoring
spec:
  rules:
  - host: alertmanager.monitoring.k8s.local
    http:
      paths:
      - backend:
          serviceName: alertmanager-main
          servicePort: 9093
[root@k8s-master01 manifests]# kubectl apply -f ingress-all-svc.yml
# 可以看到三个服务对应域名已经创建
[root@k8s-1 manifests]# kubectl get ingress -n monitoring
NAME               HOSTS                               ADDRESS   PORTS   AGE
alertmanager-ing   alertmanager.monitoring.k8s.local             80      7s
grafana-ing        grafana.monitoring.k8s.local                  80      7s
prometheus-ing     prometheus.monitoring.k8s.local               80      7s
## 查看ingress暴露svc端口，后面访问需要加上端口号
[root@k8s-master01 manifests]# kubectl get svc -n ingress-nginx
NAME            TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx   NodePort   10.254.102.184   &lt;none&gt;        80:33848/TCP,443:45891/TCP   4d3h

</code></pre>

<h2 id="4-测试">4、测试</h2>

<h3 id="4-1-访问">4.1、访问</h3>

<pre><code>配置本地host解析
10.200.46.42 prometheus.monitoring.k8s.local
10.200.46.42 grafana.monitoring.k8s.local
10.200.46.42 alertmanager.monitoring.k8s.local

浏览器打开 http://prometheus.monitoring.k8s.local,可以看到已经监控的主机及pod
</code></pre>

<p><img style="zoom:40%" src="/images/k8s/prom02.png"></p>

<h3 id="4-2-访问">4.2、访问</h3>

<pre><code>这里部署好后，查看prometheus target界面，看到kube-controller-manager、kube-scheduler目标主机都为0
</code></pre>

<p><img style="zoom:65%" src="/images/k8s/prom03.png"></p>

<pre><code>原因分析：
　　这是因为serviceMonitor选择svc时，是根据labels标签选取，而在指定的命名空间(kube-system)，并没有对应的标签。kube-apiserver之所以正常是因为kube-apiserver 服务 namespace 在default使用默认svc kubernetes。其余组件服务在kube-system 空间 ，需要单独创建svc。

# 查看serviceMonitor选取svc规则
[root@k8s-master01 manifests]# grep -2 selector serviceMonitor/prometheus-serviceMonitorKube*
serviceMonitor/prometheus-serviceMonitorKubeControllerManager.yaml-    matchNames:
serviceMonitor/prometheus-serviceMonitorKubeControllerManager.yaml-    - kube-system
serviceMonitor/prometheus-serviceMonitorKubeControllerManager.yaml:  selector:
serviceMonitor/prometheus-serviceMonitorKubeControllerManager.yaml-    matchLabels:
serviceMonitor/prometheus-serviceMonitorKubeControllerManager.yaml-      k8s-app: kube-controller-manager
--
serviceMonitor/prometheus-serviceMonitorKubelet.yaml-    matchNames:
serviceMonitor/prometheus-serviceMonitorKubelet.yaml-    - kube-system
serviceMonitor/prometheus-serviceMonitorKubelet.yaml:  selector:
serviceMonitor/prometheus-serviceMonitorKubelet.yaml-    matchLabels:
serviceMonitor/prometheus-serviceMonitorKubelet.yaml-      k8s-app: kubelet
--
serviceMonitor/prometheus-serviceMonitorKubeScheduler.yaml-    matchNames:
serviceMonitor/prometheus-serviceMonitorKubeScheduler.yaml-    - kube-system
serviceMonitor/prometheus-serviceMonitorKubeScheduler.yaml:  selector:
serviceMonitor/prometheus-serviceMonitorKubeScheduler.yaml-    matchLabels:
serviceMonitor/prometheus-serviceMonitorKubeScheduler.yaml-      k8s-app: kube-scheduler
##查看kube-system命名空间下的svc，可以看到并没有kube-scheduler、kube-controller-manager
[root@k8s-1 ~]# kubectl -n kube-system get svc
NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
kube-dns               ClusterIP   10.68.0.2       &lt;none&gt;        53/UDP,53/TCP,9153/TCP   24d
kubelet                ClusterIP   None            &lt;none&gt;        10250/TCP                22d
kubernetes-dashboard   NodePort    10.68.156.247   &lt;none&gt;        443:45454/TCP            24d
metrics-server         ClusterIP   10.68.117.89    &lt;none&gt;        443/TCP                  23d

但是却有对应的ep(没有带任何label)被创建，另外如果你的集群是kubeadm部署的就没有kubelet的ep，二进制部署的会有。
[root@k8s-1 ~]# kubectl get ep -n kube-system
NAME                      ENDPOINTS                                                               AGE
kube-controller-manager   &lt;none&gt;                                                                  27d
kube-dns                  172.30.190.1:53,172.30.197.193:53,172.30.72.129:53 + 6 more...          24d
kube-scheduler            &lt;none&gt;                                                                  27d
kubelet                   10.200.46.39:10255,10.200.46.40:10255,10.200.46.41:10255 + 12 more...   22d
kubernetes-dashboard      172.30.83.66:8443                                                       24d
metrics-server            172.30.19.197:443                                                       23d

解决：
　1、创建kube-controller-manager、kube-scheduler两个组件服务的集群svc，需要打上对应的标签，使其可以被servicemonitor选中。
[root@k8s-1 manifests]# cat controller-scheduler-svc.yml
apiVersion: v1
kind: Service
metadata:
  namespace: kube-system
  name: kube-controller-manager
  labels:
    k8s-app: kube-controller-manager
spec:
  selector:
    component: kube-controller-manager
  type: ClusterIP
  clusterIP: None
  ports:
  - name: http-metrics
    port: 10252
    targetPort: 10252
    protocol: TCP
---
apiVersion: v1
kind: Service
metadata:
  namespace: kube-system
  name: kube-scheduler
  labels:
    k8s-app: kube-scheduler
spec:
  selector:
    component: kube-scheduler
  type: ClusterIP
  clusterIP: None
  ports:
  - name: http-metrics
    port: 10251
    targetPort: 10251
    protocol: TCP
[root@k8s-1 manifests]# cat controller-scheduler-ep.yml
apiVersion: v1
kind: Endpoints
metadata:
  labels:
    k8s-app: kube-controller-manager
  name: kube-controller-manager
  namespace: kube-system
subsets:
- addresses:
  - ip: 10.200.46.39
  - ip: 10.200.46.40
  - ip: 10.200.46.41
  ports:
  - name: http-metrics
    port: 10252
    protocol: TCP
---
apiVersion: v1
kind: Endpoints
metadata:
  labels:
    k8s-app: kube-scheduler
  name: kube-scheduler
  namespace: kube-system
subsets:
- addresses:
  - ip: 10.200.46.39
  - ip: 10.200.46.40
  - ip: 10.200.46.41
  ports:
  - name: http-metrics
    port: 10251
    protocol: TCP

[root@k8s-1 manifests]# kubectl create -f controller-scheduler-svc.yml
service/kube-controller-manager created
service/kube-scheduler created
[root@k8s-1 manifests]# kubectl apply -f controller-scheduler-ep.yml
Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply
endpoints/kube-controller-manager configured
Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply
endpoints/kube-scheduler configured
[root@k8s-1 manifests]# kubectl get ep,svc -n kube-system
NAME                                ENDPOINTS                                                               AGE
endpoints/kube-controller-manager   10.200.46.39:10252,10.200.46.40:10252,10.200.46.41:10252                94s
endpoints/kube-dns                  172.30.190.1:53,172.30.197.193:53,172.30.72.129:53 + 6 more...          25d
endpoints/kube-scheduler            10.200.46.39:10251,10.200.46.40:10251,10.200.46.41:10251                94s
endpoints/kubelet                   10.200.46.39:10255,10.200.46.40:10255,10.200.46.41:10255 + 12 more...   22d
endpoints/kubernetes-dashboard      172.30.83.66:8443                                                       24d
endpoints/metrics-server            172.30.19.197:443                                                       23d

NAME                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
service/kube-dns               ClusterIP   10.68.0.2       &lt;none&gt;        53/UDP,53/TCP,9153/TCP   25d
service/kubelet                ClusterIP   None            &lt;none&gt;        10250/TCP                22d
service/kubernetes-dashboard   NodePort    10.68.156.247   &lt;none&gt;        443:45454/TCP            24d
service/metrics-server         ClusterIP   10.68.117.89    &lt;none&gt;        443/TCP                  23d

2、修改kube-controller-manager、kube-scheduler监听地址使其能访问 metrics。
## 修改为0.0.0.0
--address=0.0.0.0
修改完，重启服务，再次查看prometheus targets界面，可以看到都已经正常监听目标主机服务
</code></pre>

<p><img style="zoom:40%" src="/images/k8s/prom04.png"></p>

<h2 id="5-grafana监控查看">5、grafana监控查看</h2>

<pre><code>可以看到grafana已经有多个dashboard页面
</code></pre>

<p><img style="zoom:48%" src="/images/k8s/prom05.png">
<img style="zoom:45%" src="/images/k8s/prom06.png"></p>

            </div>
        </div>
        <div class="row">
            <div class="col-md-12">
                <hr>
            </div>
        </div>
        <section id="comments" class="themeform">
<div class="ds-thread" data-thread-key="/k8s/k8s8.5/" data-title="K8s 二进制部署8.5（部署 Prometheus-operator监控）" data-url="https://SomethingCM.github.io/k8s/k8s8.5/"></div>
<script type="text/javascript">
var duoshuoQuery = {short_name:"Zen"};
    (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] 
         || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
</script>
</section>
    </div>
    <div class="col-md-3 panel panel-success">
         <ul id="tree" class="ztree" style="list-style-type:none"></ul>
    </div>
</div>
<link rel="stylesheet" href="https://SomethingCM.github.io//css/zTreeStyle.css" type="text/css"><script src="https://SomethingCM.github.io//js/jquery.ztree.core-3.5.min.js"></script><script src="https://SomethingCM.github.io//js/ztree_toc.js"></script><script>
$(document).ready(function(){
    $('#tree').ztree_toc({
        is_posion_top:false,
        is_expand_all: true
    });
});
</script>
    <div class="container">
        <div class="row col-md-12">
            <footer>
                <div class="pull-left">
                    <p>
                        &copy;  ~ Powered By <a href="http://hugo.spf13.com">Hugo</a> - version: 0.58.3 ~ <a href="https://SomethingCM.github.io//license">License</a>
                    </p>
                </div>

                
                <div class="pull-right">
                    
                    
                    
                    
                        <a href="https://github.com/SomethingCM" target="_blank">
                        <span class="sr-only">GitHub profile</span>
                        <i class="fab fa-github-square fa-2x"></i></a>
                    
                    
                    
                    
                    
                </div>
            </footer>
        </div>
    </div>

    
    </body>
</html>


